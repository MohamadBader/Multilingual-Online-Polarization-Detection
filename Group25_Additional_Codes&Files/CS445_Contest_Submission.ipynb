{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Paths + load model"
      ],
      "metadata": {
        "id": "LqiylTJ6L-u7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOmHYcs9LxLy",
        "outputId": "18603dcf-84cf-4009-9a38-c96cb38a433c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "GPU available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/student_xlmr_base_distill/FINAL_TRAIN/FINAL' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os, glob, pandas as pd, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "DEV_DIR   = \"/content/drive/MyDrive/dev\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/student_xlmr_base_distill/FINAL_TRAIN/FINAL\"  # your final student\n",
        "OUT_ROOT  = \"/content/drive/MyDrive/submission\"\n",
        "OUT_DIR   = os.path.join(OUT_ROOT, \"subtask_1\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device).eval()\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH = 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Predict one file"
      ],
      "metadata": {
        "id": "qdWFnnO1L8E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_df(df):\n",
        "    texts = df[\"text\"].astype(str).tolist()\n",
        "    ids   = df[\"id\"].astype(str).tolist()\n",
        "\n",
        "    preds = []\n",
        "    for i in range(0, len(texts), BATCH):\n",
        "        batch_texts = texts[i:i+BATCH]\n",
        "        enc = tok(batch_texts, truncation=True, max_length=MAX_LEN, padding=True, return_tensors=\"pt\")\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "        logits = model(**enc).logits\n",
        "        y = torch.argmax(logits, dim=-1).detach().cpu().numpy().astype(int)\n",
        "        preds.extend(y.tolist())\n",
        "\n",
        "    out = pd.DataFrame({\"id\": ids, \"polarization\": preds})\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "IYenLSTlL1G2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Run all languages + save + zip"
      ],
      "metadata": {
        "id": "tEosC0jCL5G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "paths = sorted(glob.glob(os.path.join(DEV_DIR, \"*.csv\")))\n",
        "print(\"Found dev files:\", len(paths))\n",
        "assert len(paths) > 0, \"No dev csvs found. Check DEV_DIR path.\"\n",
        "\n",
        "for p in paths:\n",
        "    lang = os.path.splitext(os.path.basename(p))[0]  # e.g., \"arb\"\n",
        "    df = pd.read_csv(p, low_memory=False)\n",
        "\n",
        "    # must have id + text\n",
        "    assert \"id\" in df.columns and \"text\" in df.columns, f\"Missing id/text in {p}\"\n",
        "\n",
        "    pred = predict_df(df)\n",
        "    out_path = os.path.join(OUT_DIR, f\"pred_{lang}.csv\")\n",
        "    pred.to_csv(out_path, index=False)\n",
        "    print(\"Wrote:\", out_path, \"rows:\", len(pred))\n",
        "\n",
        "# zip folder as required\n",
        "zip_path = os.path.join(OUT_ROOT, \"subtask_1.zip\")\n",
        "if os.path.exists(zip_path):\n",
        "    os.remove(zip_path)\n",
        "\n",
        "shutil.make_archive(base_name=os.path.join(OUT_ROOT, \"subtask_1\"), format=\"zip\", root_dir=OUT_ROOT, base_dir=\"subtask_1\")\n",
        "print(\"Zipped:\", zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8xX9rI8L29m",
        "outputId": "44fd7aca-04d5-4dd4-fa08-e1cf57869af7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dev files: 22\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_amh.csv rows: 166\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_arb.csv rows: 169\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_ben.csv rows: 166\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_deu.csv rows: 159\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_eng.csv rows: 160\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_fas.csv rows: 164\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_hau.csv rows: 182\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_hin.csv rows: 137\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_ita.csv rows: 166\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_khm.csv rows: 332\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_mya.csv rows: 144\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_nep.csv rows: 100\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_ori.csv rows: 118\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_pan.csv rows: 100\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_pol.csv rows: 119\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_rus.csv rows: 167\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_spa.csv rows: 165\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_swa.csv rows: 349\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_tel.csv rows: 118\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_tur.csv rows: 115\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_urd.csv rows: 177\n",
            "Wrote: /content/drive/MyDrive/submission/subtask_1/pred_zho.csv rows: 214\n",
            "Zipped: /content/drive/MyDrive/submission/subtask_1.zip\n"
          ]
        }
      ]
    }
  ]
}