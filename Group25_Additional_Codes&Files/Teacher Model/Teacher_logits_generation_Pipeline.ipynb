{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "print(\"unmounted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuB77cuw2hOI",
        "outputId": "7a02c0e8-3dd0-4717-f912-d6c6da95becb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "unmounted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "shutil.rmtree(\"/content/drive\", ignore_errors=True)\n",
        "os.makedirs(\"/content/drive\", exist_ok=True)\n",
        "print(\"cleared /content/drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnmm0kHZ2iSR",
        "outputId": "39569c59-0174-46da-b3e7-f7b81d966724"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleared /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcEA1ENs2kKe",
        "outputId": "d2ca3c6d-76ea-4213-ea49-33bdb28b03a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# INSPECT MASTER + SHARD FILES\n",
        "# =========================\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "SHARDS_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "\n",
        "print(\"=== MASTER INSPECTION ===\")\n",
        "master = pd.read_csv(MASTER_IN, low_memory=False)\n",
        "print(\"Master shape:\", master.shape)\n",
        "print(\"Master columns:\", list(master.columns))\n",
        "\n",
        "# show key columns presence\n",
        "for c in [\"id\", \"row_id\", \"lang\", \"split\", \"text\", \"text_en\"]:\n",
        "    print(f\"Master has {c}:\", c in master.columns)\n",
        "\n",
        "print(\"\\nMaster split counts:\")\n",
        "print(master[\"split\"].value_counts(dropna=False).to_string() if \"split\" in master.columns else \"NO split col\")\n",
        "\n",
        "print(\"\\nMaster lang counts (top 15):\")\n",
        "print(master[\"lang\"].value_counts().head(15).to_string() if \"lang\" in master.columns else \"NO lang col\")\n",
        "\n",
        "# quick sample\n",
        "show_cols = [c for c in [\"id\",\"row_id\",\"lang\",\"split\",\"text\",\"polarized\"] if c in master.columns]\n",
        "print(\"\\nMaster sample (3 rows):\")\n",
        "print(master[show_cols].head(3).to_string(index=False))\n",
        "\n",
        "print(\"\\n=== SHARD FILE DISCOVERY ===\")\n",
        "shard_paths = sorted(glob.glob(os.path.join(SHARDS_DIR, \"shard_person*_S*_*.csv\")))\n",
        "print(\"Found shard CSVs:\", len(shard_paths))\n",
        "print(\"Shard filenames:\")\n",
        "for p in shard_paths:\n",
        "    print(\" -\", os.path.basename(p))\n",
        "\n",
        "assert len(shard_paths) > 0, \"No shard CSVs found.\"\n",
        "\n",
        "print(\"\\n=== SHARD COLUMN CHECK (first 3 files) ===\")\n",
        "for p in shard_paths[:3]:\n",
        "    df = pd.read_csv(p, low_memory=False)\n",
        "    print(\"\\nFile:\", os.path.basename(p))\n",
        "    print(\"  shape:\", df.shape)\n",
        "    print(\"  columns:\", list(df.columns))\n",
        "    for c in [\"id\", \"row_id\", \"lang\", \"text\", \"text_en\", \"translation\", \"translated_text\", \"output\"]:\n",
        "        if c in df.columns:\n",
        "            print(\"  has:\", c)\n",
        "\n",
        "print(\"\\n=== SHARD KEY SANITY (all shard files) ===\")\n",
        "summary = []\n",
        "for p in shard_paths:\n",
        "    df = pd.read_csv(p, low_memory=False)\n",
        "    cols = set(df.columns)\n",
        "    key = \"row_id\" if \"row_id\" in cols else (\"id\" if \"id\" in cols else None)\n",
        "    out = \"text_en\" if \"text_en\" in cols else (\"translation\" if \"translation\" in cols else (\"translated_text\" if \"translated_text\" in cols else None))\n",
        "    summary.append([os.path.basename(p), df.shape[0], df.shape[1], key, out])\n",
        "summary_df = pd.DataFrame(summary, columns=[\"file\",\"rows\",\"cols\",\"key_col\",\"text_en_col_guess\"])\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== SHARD SAMPLE ROWS (pick one file) ===\")\n",
        "pick = shard_paths[0]\n",
        "df0 = pd.read_csv(pick, low_memory=False)\n",
        "print(\"Picked:\", os.path.basename(pick))\n",
        "# print a compact view of relevant cols\n",
        "cand = [c for c in [\"id\",\"row_id\",\"lang\",\"text\",\"text_en\",\"translation\",\"translated_text\"] if c in df0.columns]\n",
        "print(df0[cand].head(5).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFDyGbyht6HR",
        "outputId": "7128fa72-6afe-453b-d987-8d5664b49d40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MASTER INSPECTION ===\n",
            "Master shape: (77368, 10)\n",
            "Master columns: ['id', 'text', 'political', 'racial/ethnic', 'religious', 'gender/sexual', 'other', 'lang', 'split', 'polarized']\n",
            "Master has id: True\n",
            "Master has row_id: False\n",
            "Master has lang: True\n",
            "Master has split: True\n",
            "Master has text: True\n",
            "Master has text_en: False\n",
            "\n",
            "Master split counts:\n",
            "split\n",
            "train    73681\n",
            "dev       3687\n",
            "\n",
            "Master lang counts (top 15):\n",
            "lang\n",
            "swa    7340\n",
            "khm    6972\n",
            "zho    4494\n",
            "hau    3833\n",
            "urd    3740\n",
            "arb    3549\n",
            "rus    3515\n",
            "ita    3500\n",
            "ben    3499\n",
            "amh    3498\n",
            "spa    3470\n",
            "fas    3459\n",
            "eng    3382\n",
            "deu    3339\n",
            "mya    3033\n",
            "\n",
            "Master sample (3 rows):\n",
            "                                  id lang split                                                                                                text  polarized\n",
            "spa_bc7bf0a1b710cd724cf96b5eeb020bff  spa train \"bueno, tirando\"\\ny si hay repregunta, entonces palante.\\nprotestante no se. proteston soy un rato.        0.0\n",
            "spa_688aec0f6abf4e0be44ecbfa886251cc  spa train                                                                 \"caimos en su retorica de indigena\"        0.0\n",
            "spa_47019efabe7295c630bcc04bd0429230  spa train                                                    \"cara de indigena sudaca\", porque pio asi luego?        0.0\n",
            "\n",
            "=== SHARD FILE DISCOVERY ===\n",
            "Found shard CSVs: 8\n",
            "Shard filenames:\n",
            " - shard_person1_S0_khm_nep_tel.csv\n",
            " - shard_person1_S1_khm_nep_tel.csv\n",
            " - shard_person2_S0_ben_amh_ita.csv\n",
            " - shard_person2_S1_ben_amh_ita.csv\n",
            " - shard_person3_S0_swa_ori_pan.csv\n",
            " - shard_person3_S1_swa_ori_pan.csv\n",
            " - shard_person4_S0_zho_mya_fas.csv\n",
            " - shard_person4_S1_zho_mya_fas.csv\n",
            "\n",
            "=== SHARD COLUMN CHECK (first 3 files) ===\n",
            "\n",
            "File: shard_person1_S0_khm_nep_tel.csv\n",
            "  shape: (5389, 5)\n",
            "  columns: ['row_id', 'lang', 'split', 'text', 'text_en']\n",
            "  has: row_id\n",
            "  has: lang\n",
            "  has: text\n",
            "  has: text_en\n",
            "\n",
            "File: shard_person1_S1_khm_nep_tel.csv\n",
            "  shape: (5465, 5)\n",
            "  columns: ['row_id', 'lang', 'split', 'text', 'text_en']\n",
            "  has: row_id\n",
            "  has: lang\n",
            "  has: text\n",
            "  has: text_en\n",
            "\n",
            "File: shard_person2_S0_ben_amh_ita.csv\n",
            "  shape: (4756, 5)\n",
            "  columns: ['row_id', 'lang', 'split', 'text', 'text_en']\n",
            "  has: row_id\n",
            "  has: lang\n",
            "  has: text\n",
            "  has: text_en\n",
            "\n",
            "=== SHARD KEY SANITY (all shard files) ===\n",
            "                            file  rows  cols key_col text_en_col_guess\n",
            "shard_person1_S0_khm_nep_tel.csv  5389     5  row_id           text_en\n",
            "shard_person1_S1_khm_nep_tel.csv  5465     5  row_id           text_en\n",
            "shard_person2_S0_ben_amh_ita.csv  4756     5  row_id           text_en\n",
            "shard_person2_S1_ben_amh_ita.csv  4639     5  row_id           text_en\n",
            "shard_person3_S0_swa_ori_pan.csv  5406     5  row_id           text_en\n",
            "shard_person3_S1_swa_ori_pan.csv  5383     5  row_id           text_en\n",
            "shard_person4_S0_zho_mya_fas.csv  5029     5  row_id           text_en\n",
            "shard_person4_S1_zho_mya_fas.csv  5091     5  row_id           text_en\n",
            "\n",
            "=== SHARD SAMPLE ROWS (pick one file) ===\n",
            "Picked: shard_person1_S0_khm_nep_tel.csv\n",
            "          row_id lang                                                                                                                                                                                                                                                                                                                                                                            text                                                                                                                                                                                                                                                                                                                                                                                                        text_en\n",
            "a3c780d72560856c  khm                                                                                                                ការឃុំខ្លួនកញ្ញា សេង ធារី កាន់តែយូរដោយរដ្ឋាភិបាលលោក ហ៊ុន សែន នោះនឹងធ្វើឱ្យមនុស្សម្នាកាន់តែច្រើនដឹងឮកាន់តែខ្លាំងអំពីរបបផ្តាច់ការនេះ និងការដែលរបបនេះពុំមានឆន្ទៈអនុញ្ញាតឱ្យមានសំឡេងនយោបាយប្រឆាំង ព្រមទាំងវិធីដែលលោក ហ៊ុន សែន រក្សាអំណាចដោយប្រើកណ្តាប់ដៃដែករបស់គាត់។                                                                                                                                                 The prolonged detention of Ms. Seng Thary by the government of Mr. Hun Sen will only make more people aware of this authoritarian regime and how it suppresses any opposing political voices, as well as the iron grip with which Mr. Hun Sen maintains power.\n",
            "9b2f6f45979c2732  khm                                សាលាដំបូងរាជធានីភ្នំពេញបានចោទប្រកាន់លោក សុន ឆ័យ ពីបទបរិហាកេរ្តិ៍ពាក់ព័ន្ធនឹងការអត្ថាធិប្បាយរិះគន់ការបោះឆ្នោតក្រុមប្រឹក្សាឃុំសង្កាត់ តាមពាក្យបណ្តឹងរបស់គណបក្សប្រជាជនកម្ពុជា ដែលទាមទារឱ្យមេដឹកនាំគណបក្សប្រឆាំងបង់សងជំងឺចិត្តចំនួន ៤ ពាន់លានរៀល គឺប្រមាណ ១ លានដុល្លារអាមេរិក និងស្ថាប័នរៀបចំការបោះឆ្នោត គ.ជ.ប ទាមទារឱ្យលោក សុន ឆ័យ សុំទោសជាសាធារណៈ។ The Phnom Penh Municipal Court has charged Mr. Sun Chhay with defamation related to comments criticizing the commune council elections, based on a complaint from the Cambodian People's Party, which demands that the opposition party leader pay a fine of 4,000,000 riels, approximately 1 million US dollars, and that the election organizing body, the NEC, demands a public apology from Mr. Sun Chhay.\n",
            "0faf3edc4ac02746  khm ក្នុងលិខិតបានបញ្ជាក់ថា ក្រុមការងារបង្ក្រាបមានតួនាទីភារកិច្ចក្នុងការរៀបចំផែនការ វិធានការ គោលការណ៍ណែនាំ ដើម្បីឱ្យអាជ្ញាធរមានសមត្ថកិច្ចគ្រប់លំដាប់ថ្នាក់ អនុវត្តបង្ក្រាបនូវរាល់ការលេងល្បែងស៊ីសងខុសច្បាប់គ្រប់ប្រភេទ និងត្រូវសហការជាមួយអាជ្ញាធរមានសមត្ថកិច្ចទាំងនៅថ្នាក់ជាតិ រដ្ឋបាលថ្នាក់ក្រោមជាតិ ដើម្បីធ្វើការស្រាវជ្រាវ និងចាត់វិធានការបង្ក្រាបនូវទីតាំងលេងល្បែងស៊ីសងខុសច្បាប់។                                                                  The letter states that the enforcement team has a duty to prepare plans and guidelines for authorities at all levels to enforce against all forms of illegal gambling and to cooperate with relevant authorities at both the national and sub-national levels to investigate and take enforcement actions against illegal gambling locations.\n",
            "e34d94fdcdbbe510  khm                                                                                                                                                                                                                                                               ខ្ញុំនៅតែនឹក នៅតែស្រលាញ់គេ ប៉ុន្តែពេលវេលាបណ្តាលឲ្យខ្ញុំបានស្គាល់មនុស្សល្អម្នាក់ទៀត ដែលផ្តល់រសជាតិជីវិតមួយបែបផ្សេង                                                                                                                                                                                                                                                                                 I still think of them, still love them, but time has led me to meet another good person who brings a different flavor to life.\n",
            "6cfbc7b1fc65b94c  khm                                                                                                                                                                                                                        ទំនាយទាយថា លោកអ្នកមានរឿងរ៉ាវជាច្រើនដែលត្រូវដោះស្រាយ ប៉ុន្តែបើទោះបីជាដោះមិនបានល្អក្តី ក៏លោកអ្នកនៅតែរក្សាស្ថិរភាពបានជាធម្មតា មិនបង្កបញ្ហាធ្ងន់ធ្ងរអ្វីឡើយ។                                                                                                                                                                                                                                                  It seems you have many issues to resolve, but even if they aren't resolved well, you still manage to maintain stability without causing any serious problems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, glob, os\n",
        "\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "SHARDS_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "\n",
        "master = pd.read_csv(MASTER_IN, low_memory=False)\n",
        "\n",
        "# load one shard\n",
        "p = sorted(glob.glob(os.path.join(SHARDS_DIR, \"shard_person*_S*_*.csv\")))[0]\n",
        "sh = pd.read_csv(p, low_memory=False)\n",
        "\n",
        "print(\"Master id example:\", master[\"id\"].iloc[0])\n",
        "print(\"Shard row_id example:\", sh[\"row_id\"].iloc[0])\n",
        "\n",
        "# key overlap check\n",
        "master_ids = set(master[\"id\"].astype(str))\n",
        "shard_row_ids = set(sh[\"row_id\"].astype(str))\n",
        "\n",
        "overlap = len(master_ids & shard_row_ids)\n",
        "print(\"Overlap count (master.id ∩ shard.row_id):\", overlap)\n",
        "print(\"Overlap % of shard:\", overlap / len(shard_row_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAqE81h7uNLU",
        "outputId": "c0fa7fe0-2acb-4831-d84e-744c644f599a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master id example: spa_bc7bf0a1b710cd724cf96b5eeb020bff\n",
            "Shard row_id example: a3c780d72560856c\n",
            "Overlap count (master.id ∩ shard.row_id): 0\n",
            "Overlap % of shard: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "master = pd.read_csv(\"/content/drive/MyDrive/master_dataset.csv\", low_memory=False)\n",
        "\n",
        "dup = master.duplicated(subset=[\"lang\",\"split\",\"text\"]).sum()\n",
        "print(\"Master duplicates by (lang,split,text):\", dup)\n",
        "\n",
        "if dup > 0:\n",
        "    # show a few duplicates\n",
        "    d = master[master.duplicated(subset=[\"lang\",\"split\",\"text\"], keep=False)] \\\n",
        "              .sort_values([\"lang\",\"split\",\"text\"]) \\\n",
        "              .head(10)\n",
        "    print(d[[\"id\",\"lang\",\"split\",\"text\"]].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JDyrrsAuR6h",
        "outputId": "9af6d6a3-5904-4af6-d9ca-a337949285d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master duplicates by (lang,split,text): 3\n",
            "                                  id lang split                                                                                                                  text\n",
            "ita_4aabf598b2b7663c2912407ef5a57d63  ita train                                     Bici rubate ai ciclisti della Milano-Saremo: trovate in un campo nomadi @URL @URL\n",
            "ita_3f40e90b06195d56f6b83aece1c63ad8  ita train                                     Bici rubate ai ciclisti della Milano-Saremo: trovate in un campo nomadi @URL @URL\n",
            "ita_32d6701aaedb97e4f6fa87596554b13e  ita train Qui Radio Londra: Roma, cinese morta, ore contate per chi l'ha scippata: «Vengono dal campo rom» - Il Messaggero @URL\n",
            "ita_e21c9d831fd82aba4d0436174505dece  ita train Qui Radio Londra: Roma, cinese morta, ore contate per chi l'ha scippata: «Vengono dal campo rom» - Il Messaggero @URL\n",
            "ita_4c67b8940fd4c4c49849110f58f70d97  ita train                                                                     Roma, quei podisti perseguitati dai rom @URL @URL\n",
            "ita_1ea3cbdb0ff04ba94d4aa3489867c382  ita train                                                                     Roma, quei podisti perseguitati dai rom @URL @URL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling Master text_en"
      ],
      "metadata": {
        "id": "_gz3AyRUEJql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import pandas as pd\n",
        "\n",
        "MASTER_IN  = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "SHARDS_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "MASTER_OUT = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"\n",
        "\n",
        "OUT_COL  = \"text_en\"\n",
        "LANG_COL = \"lang\"\n",
        "SPLIT_COL= \"split\"\n",
        "TEXT_COL = \"text\"\n",
        "\n",
        "FILL_ONLY_MISSING = True   # keep True\n",
        "\n",
        "# 1) Load master\n",
        "master = pd.read_csv(MASTER_IN, low_memory=False)\n",
        "\n",
        "# ensure text_en exists\n",
        "if OUT_COL not in master.columns:\n",
        "    master[OUT_COL] = pd.NA\n",
        "\n",
        "before = master[OUT_COL].notna().sum()\n",
        "\n",
        "# 2) Load all shard CSVs\n",
        "shard_paths = sorted(glob.glob(os.path.join(SHARDS_DIR, \"shard_person*_S*_*.csv\")))\n",
        "print(\"Found shard CSVs:\", len(shard_paths))\n",
        "assert len(shard_paths) > 0, \"No shard CSVs found.\"\n",
        "\n",
        "shards = []\n",
        "for p in shard_paths:\n",
        "    df = pd.read_csv(p, low_memory=False)\n",
        "\n",
        "    # keep only needed columns\n",
        "    need = [LANG_COL, SPLIT_COL, TEXT_COL, OUT_COL]\n",
        "    missing_cols = [c for c in need if c not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(\"[SKIP]\", os.path.basename(p), \"missing\", missing_cols)\n",
        "        continue\n",
        "\n",
        "    df = df[need].copy()\n",
        "\n",
        "    # clean translations\n",
        "    df[OUT_COL] = df[OUT_COL].astype(str).replace({\"nan\": \"\"}).str.strip()\n",
        "    df = df[df[OUT_COL].notna() & (df[OUT_COL] != \"\")]\n",
        "\n",
        "    shards.append(df)\n",
        "\n",
        "all_shards = pd.concat(shards, ignore_index=True)\n",
        "print(\"Shard rows (after dropping empty text_en):\", len(all_shards))\n",
        "\n",
        "# 3) Deduplicate shards on the merge key\n",
        "all_shards = all_shards.drop_duplicates(subset=[LANG_COL, SPLIT_COL, TEXT_COL], keep=\"last\")\n",
        "print(\"Shard unique (lang,split,text):\", len(all_shards))\n",
        "\n",
        "# 4) Merge into master on (lang, split, text)\n",
        "merged = master.merge(\n",
        "    all_shards,\n",
        "    on=[LANG_COL, SPLIT_COL, TEXT_COL],\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_shard\"),\n",
        ")\n",
        "\n",
        "# 5) Fill master text_en\n",
        "if FILL_ONLY_MISSING:\n",
        "    miss = merged[OUT_COL].isna() | (merged[OUT_COL].astype(str).str.strip() == \"\")\n",
        "    merged.loc[miss, OUT_COL] = merged.loc[miss, OUT_COL + \"_shard\"]\n",
        "else:\n",
        "    merged[OUT_COL] = merged[OUT_COL + \"_shard\"].combine_first(merged[OUT_COL])\n",
        "\n",
        "merged = merged.drop(columns=[OUT_COL + \"_shard\"])\n",
        "\n",
        "after = merged[OUT_COL].notna().sum()\n",
        "print(\"Master text_en filled before:\", before)\n",
        "print(\"Master text_en filled after :\", after)\n",
        "print(\"Newly filled:\", after - before)\n",
        "\n",
        "# 6) Remaining missing report\n",
        "missing = merged[OUT_COL].isna() | (merged[OUT_COL].astype(str).str.strip() == \"\")\n",
        "print(\"\\nRemaining missing text_en:\", int(missing.sum()))\n",
        "print(\"Missing by lang (top 20):\")\n",
        "print(merged.loc[missing, LANG_COL].value_counts().head(20).to_string())\n",
        "\n",
        "# 7) Save\n",
        "merged.to_csv(MASTER_OUT, index=False)\n",
        "print(\"\\nSaved:\", MASTER_OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NHXZvPzu3w-",
        "outputId": "eb70310c-36ea-483d-839d-db1da90ce14b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found shard CSVs: 8\n",
            "Shard rows (after dropping empty text_en): 41158\n",
            "Shard unique (lang,split,text): 41155\n",
            "Master text_en filled before: 0\n",
            "Master text_en filled after : 41158\n",
            "Newly filled: 41158\n",
            "\n",
            "Remaining missing text_en: 36210\n",
            "Missing by lang (top 20):\n",
            "lang\n",
            "hau    3833\n",
            "urd    3740\n",
            "arb    3549\n",
            "rus    3515\n",
            "spa    3470\n",
            "eng    3382\n",
            "deu    3339\n",
            "hin    2881\n",
            "pol    2510\n",
            "tur    2479\n",
            "ita     573\n",
            "swa     573\n",
            "fas     440\n",
            "amh     362\n",
            "khm     334\n",
            "tel     273\n",
            "zho     214\n",
            "mya     212\n",
            "ben     167\n",
            "pan     132\n",
            "\n",
            "Saved: /content/drive/MyDrive/master_dataset_with_text_en.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## copy teacher languages into text_en"
      ],
      "metadata": {
        "id": "TbIdKQLTvPAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"\n",
        "df = pd.read_csv(PATH, low_memory=False)\n",
        "\n",
        "TEACHER_LANGS = ['eng','spa','deu','rus','tur','pol','arb']\n",
        "\n",
        "missing = df[\"text_en\"].isna() | (df[\"text_en\"].astype(str).str.strip() == \"\")\n",
        "mask_teacher = df[\"lang\"].isin(TEACHER_LANGS)\n",
        "\n",
        "before = (~missing).sum()\n",
        "df.loc[mask_teacher & missing, \"text_en\"] = df.loc[mask_teacher & missing, \"text\"]\n",
        "\n",
        "missing2 = df[\"text_en\"].isna() | (df[\"text_en\"].astype(str).str.strip() == \"\")\n",
        "after = (~missing2).sum()\n",
        "\n",
        "print(\"filled before:\", int(before))\n",
        "print(\"filled after :\", int(after))\n",
        "print(\"newly filled :\", int(after - before))\n",
        "\n",
        "print(\"\\nRemaining missing by lang (top 20):\")\n",
        "print(df.loc[missing2, \"lang\"].value_counts().head(20).to_string())\n",
        "\n",
        "df.to_csv(PATH, index=False)\n",
        "print(\"\\nSaved updated file:\", PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1NOKnt1vNjX",
        "outputId": "86aa395a-d6e3-4c3d-a59b-a3343c92ebdf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filled before: 41158\n",
            "filled after : 63402\n",
            "newly filled : 22244\n",
            "\n",
            "Remaining missing by lang (top 20):\n",
            "lang\n",
            "hau    3833\n",
            "urd    3740\n",
            "hin    2881\n",
            "swa     573\n",
            "ita     573\n",
            "fas     440\n",
            "amh     362\n",
            "khm     334\n",
            "tel     273\n",
            "zho     214\n",
            "mya     212\n",
            "ben     167\n",
            "pan     132\n",
            "ori     132\n",
            "nep     100\n",
            "\n",
            "Saved updated file: /content/drive/MyDrive/master_dataset_with_text_en.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"\n",
        "df = pd.read_csv(PATH, low_memory=False)\n",
        "\n",
        "miss = df[\"text_en\"].isna() | (df[\"text_en\"].astype(str).str.strip()==\"\")\n",
        "have = ~miss\n",
        "\n",
        "print(\"Missing count:\", int(miss.sum()))\n",
        "print(\"\\nLabel dist (have_en):\")\n",
        "print(df.loc[have & (df[\"split\"]==\"train\"), \"polarized\"].value_counts(normalize=True).to_string())\n",
        "\n",
        "print(\"\\nLabel dist (missing_en):\")\n",
        "print(df.loc[miss & (df[\"split\"]==\"train\"), \"polarized\"].value_counts(normalize=True).to_string())\n",
        "\n",
        "print(\"\\nText length mean (have_en vs missing_en):\")\n",
        "print(\"have_en:\", df.loc[have, \"text\"].astype(str).str.len().mean())\n",
        "print(\"missing:\", df.loc[miss, \"text\"].astype(str).str.len().mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgBFqHdmwOCt",
        "outputId": "11077532-d2d5-452d-e9a1-da2f82ea89e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing count: 13966\n",
            "\n",
            "Label dist (have_en):\n",
            "polarized\n",
            "1.0    0.534131\n",
            "0.0    0.465869\n",
            "\n",
            "Label dist (missing_en):\n",
            "polarized\n",
            "1.0    0.515574\n",
            "0.0    0.484426\n",
            "\n",
            "Text length mean (have_en vs missing_en):\n",
            "have_en: 105.64728242011293\n",
            "missing: 112.10067306315337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits Generation"
      ],
      "metadata": {
        "id": "HQY12g_-Dy6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ---------- paths ----------\n",
        "MASTER_PATH = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"\n",
        "OUT_MASTER  = \"/content/drive/MyDrive/master_with_teacherA_logits.csv\"\n",
        "LOGITS_PATH = \"/content/drive/MyDrive/distill/teacherA_logits_real_train.csv\"  # resumable\n",
        "os.makedirs(os.path.dirname(LOGITS_PATH), exist_ok=True)\n",
        "\n",
        "#TEACHER_DIR_BASE = \"/content/drive/MyDrive/teachers/teacherA_xlmr_large\"  # folder containing checkpoints\n",
        "# If you have a FINAL folder, set it instead:\n",
        "TEACHER_DIR_BASE = \"/content/drive/MyDrive/teachers/teacherA_xlmr_large_FINAL\"\n",
        "\n",
        "# ---------- pick latest checkpoint automatically (if not FINAL) ----------\n",
        "def pick_ckpt(base_dir):\n",
        "    if os.path.isfile(os.path.join(base_dir, \"config.json\")):\n",
        "        return base_dir  # already a model dir\n",
        "    ckpts = [d for d in os.listdir(base_dir) if d.startswith(\"checkpoint-\")]\n",
        "    if not ckpts:\n",
        "        raise FileNotFoundError(f\"No checkpoints found in {base_dir}\")\n",
        "    ckpts = sorted(ckpts, key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
        "    return os.path.join(base_dir, ckpts[-1])\n",
        "\n",
        "TEACHER_CKPT = pick_ckpt(TEACHER_DIR_BASE)\n",
        "print(\"Using teacher checkpoint:\", TEACHER_CKPT)\n",
        "\n",
        "# ---------- load data ----------\n",
        "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
        "\n",
        "# real + train + has text_en\n",
        "has_en = df[\"text_en\"].notna() & (df[\"text_en\"].astype(str).str.strip() != \"\")\n",
        "is_train = df[\"split\"].astype(str).eq(\"train\")\n",
        "\n",
        "# if is_synthetic exists, exclude it; otherwise master is real anyway\n",
        "if \"is_synthetic\" in df.columns:\n",
        "    is_real = df[\"is_synthetic\"].fillna(0).astype(int).eq(0)\n",
        "else:\n",
        "    is_real = pd.Series(True, index=df.index)\n",
        "\n",
        "pool = df[is_train & has_en & is_real].copy()\n",
        "pool[\"id\"] = pool[\"id\"].astype(str)\n",
        "print(\"Real train rows with text_en:\", pool.shape)\n",
        "\n",
        "# ---------- resume (skip already computed ids) ----------\n",
        "done_ids = set()\n",
        "if os.path.exists(LOGITS_PATH):\n",
        "    prev = pd.read_csv(LOGITS_PATH, usecols=[\"id\"])\n",
        "    done_ids = set(prev[\"id\"].astype(str))\n",
        "    print(\"Already have logits for:\", len(done_ids))\n",
        "\n",
        "pool = pool[~pool[\"id\"].isin(done_ids)].reset_index(drop=True)\n",
        "print(\"To compute now:\", len(pool))\n",
        "\n",
        "if len(pool) == 0:\n",
        "    print(\"Nothing to do. (All logits already computed)\")\n",
        "else:\n",
        "    # ---------- load teacher ----------\n",
        "    tok = AutoTokenizer.from_pretrained(TEACHER_CKPT)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(TEACHER_CKPT)\n",
        "    model.eval().to(\"cuda\")\n",
        "\n",
        "    MAX_LEN = 256\n",
        "    BATCH = 64  # A100 ok; reduce to 32 if you hit OOM\n",
        "\n",
        "    class TextEnDS(Dataset):\n",
        "        def __init__(self, df):\n",
        "            self.ids = df[\"id\"].tolist()\n",
        "            self.texts = df[\"text_en\"].astype(str).tolist()\n",
        "        def __len__(self): return len(self.ids)\n",
        "        def __getitem__(self, i): return self.ids[i], self.texts[i]\n",
        "\n",
        "    ds = TextEnDS(pool)\n",
        "    dl = DataLoader(ds, batch_size=BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "    rows = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            ids, texts = batch\n",
        "            enc = tok(list(texts), truncation=True, max_length=MAX_LEN, padding=True, return_tensors=\"pt\")\n",
        "            enc = {k: v.to(\"cuda\") for k, v in enc.items()}\n",
        "\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                logits = model(**enc).logits  # [bs,2]\n",
        "\n",
        "            probs = torch.softmax(logits.float(), dim=1)\n",
        "            p1 = probs[:, 1].cpu().numpy()\n",
        "            log0 = logits[:, 0].float().cpu().numpy()\n",
        "            log1 = logits[:, 1].float().cpu().numpy()\n",
        "\n",
        "            for i in range(len(ids)):\n",
        "                rows.append((str(ids[i]), float(log0[i]), float(log1[i]), float(p1[i])))\n",
        "\n",
        "    out_df = pd.DataFrame(rows, columns=[\"id\", \"teacherA_logit0\", \"teacherA_logit1\", \"teacherA_p1\"])\n",
        "\n",
        "    # append to logits file (resumable)\n",
        "    header = not os.path.exists(LOGITS_PATH)\n",
        "    out_df.to_csv(LOGITS_PATH, mode=\"a\", header=header, index=False)\n",
        "    print(\"Wrote logits:\", len(out_df), \"->\", LOGITS_PATH)\n",
        "\n",
        "# ---------- add logits columns into master and save ----------\n",
        "logits_all = pd.read_csv(LOGITS_PATH, low_memory=False)\n",
        "logits_all[\"id\"] = logits_all[\"id\"].astype(str)\n",
        "\n",
        "df[\"id\"] = df[\"id\"].astype(str)\n",
        "df2 = df.merge(logits_all, on=\"id\", how=\"left\")\n",
        "\n",
        "df2.to_csv(OUT_MASTER, index=False)\n",
        "print(\"Saved merged master with teacher logits:\", OUT_MASTER)\n",
        "print(\"Non-null teacherA_p1:\", int(df2[\"teacherA_p1\"].notna().sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMXQt_sT0gZN",
        "outputId": "e6f30339-1380-4630-d7e8-f3ff6a8346d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using teacher checkpoint: /content/drive/MyDrive/teachers/teacherA_xlmr_large_FINAL\n",
            "Real train rows with text_en: (70546, 11)\n",
            "Already have logits for: 62348\n",
            "To compute now: 8198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/teachers/teacherA_xlmr_large_FINAL' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "/tmp/ipython-input-1194108774.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote logits: 8198 -> /content/drive/MyDrive/distill/teacherA_logits_real_train.csv\n",
            "Saved merged master with teacher logits: /content/drive/MyDrive/master_with_teacherA_logits.csv\n",
            "Non-null teacherA_p1: 70546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding the last 3 langauges (took too much time during translation)"
      ],
      "metadata": {
        "id": "LQkomfuSlMEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "MASTER_IN  = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"\n",
        "MASTER_OUT = \"/content/drive/MyDrive/master_dataset_with_text_en.csv\"  # overwrite same file\n",
        "\n",
        "s0 = pd.read_csv(\"/content/drive/MyDrive/translation_shards/shard_person0_S0_urd_hin_hau.csv\", low_memory=False)\n",
        "s1 = pd.read_csv(\"/content/drive/MyDrive/translation_shards/shard_person0_S1_urd_hin_hau.csv\", low_memory=False)\n",
        "\n",
        "master = pd.read_csv(MASTER_IN, low_memory=False)\n",
        "\n",
        "# keep only needed cols\n",
        "need = [\"lang\",\"split\",\"text\",\"text_en\"]\n",
        "sh = pd.concat([s0[need], s1[need]], ignore_index=True)\n",
        "\n",
        "# drop empty text_en + dedupe by (lang,split,text)\n",
        "sh[\"text_en\"] = sh[\"text_en\"].astype(str).replace({\"nan\": \"\"}).str.strip()\n",
        "sh = sh[sh[\"text_en\"].notna() & (sh[\"text_en\"] != \"\")]\n",
        "sh = sh.drop_duplicates(subset=[\"lang\",\"split\",\"text\"], keep=\"last\")\n",
        "\n",
        "# merge + fill only missing\n",
        "merged = master.merge(sh, on=[\"lang\",\"split\",\"text\"], how=\"left\", suffixes=(\"\", \"_shard\"))\n",
        "miss = merged[\"text_en\"].isna() | (merged[\"text_en\"].astype(str).str.strip() == \"\")\n",
        "merged.loc[miss, \"text_en\"] = merged.loc[miss, \"text_en_shard\"]\n",
        "merged = merged.drop(columns=[\"text_en_shard\"])\n",
        "\n",
        "# save\n",
        "merged.to_csv(MASTER_OUT, index=False)\n",
        "print(\"Saved:\", MASTER_OUT)\n",
        "\n",
        "# sanity: missing by lang\n",
        "missing = merged[\"text_en\"].isna() | (merged[\"text_en\"].astype(str).str.strip() == \"\")\n",
        "print(\"Remaining missing total:\", int(missing.sum()))\n",
        "print(merged.loc[missing, \"lang\"].value_counts().head(20).to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pljq1JazlBkg",
        "outputId": "26e14f39-018d-4ef1-a8fc-c3b5f4c51515"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/master_dataset_with_text_en.csv\n",
            "Remaining missing total: 5768\n",
            "lang\n",
            "urd    1459\n",
            "swa     573\n",
            "ita     573\n",
            "hin     564\n",
            "fas     440\n",
            "amh     362\n",
            "khm     334\n",
            "tel     273\n",
            "hau     233\n",
            "zho     214\n",
            "mya     212\n",
            "ben     167\n",
            "pan     132\n",
            "ori     132\n",
            "nep     100\n"
          ]
        }
      ]
    }
  ]
}