{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIUvSFU524gp",
        "outputId": "5a3af1af-900f-4ef7-9a2d-b98cf0c175cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kBLw2xlK09o3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL TEAM TRANSLATION SCRIPT (DRIVE-SAFE + NO UNNEEDED API)\n",
        "# - Copy-only for TEACHER_LANGS (NO API calls)\n",
        "# - Translate only selected languages for THIS run\n",
        "# - Team sharding: 5 people + optional 2 sessions/person (2 notebooks/keys)\n",
        "# - High-quality prompts (similar to your old logic)\n",
        "# - Marker protection + validation (no new @/#/URLs)\n",
        "# - Batched translation (faster)\n",
        "# - DURABLE checkpointing to Google Drive (atomic write + fsync)\n",
        "# - Resume-safe (skips already translated row_ids in shard file)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# 1) CONFIG\n",
        "# =========================\n",
        "\n",
        "# Input master (original)\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "\n",
        "# Output directory for shards (each run writes its own file)\n",
        "OUT_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Output column name\n",
        "OUT_COL = \"text_en\"\n",
        "\n",
        "# Master columns\n",
        "TEXT_COL = \"text\"\n",
        "LANG_COL = \"lang\"\n",
        "SPLIT_COL = \"split\"\n",
        "\n",
        "# Translate only train by default (recommended)\n",
        "TRANSLATE_SPLITS = {\"train\"}  # change to {\"train\",\"dev\"} if you really want dev too\n",
        "\n",
        "# Teacher languages: copy-only (no API calls)\n",
        "TEACHER_LANGS = {\"eng\", \"spa\", \"deu\", \"rus\", \"tur\", \"pol\", \"arb\"}\n",
        "\n",
        "# Language names (for prompt)\n",
        "LANG_MAP = {\n",
        "    \"amh\": \"Amharic\", \"arb\": \"Arabic\", \"ben\": \"Bengali\", \"deu\": \"German\", \"eng\": \"English\",\n",
        "    \"fas\": \"Persian\", \"hau\": \"Hausa\", \"hin\": \"Hindi\", \"ita\": \"Italian\", \"khm\": \"Khmer\",\n",
        "    \"mya\": \"Burmese\", \"nep\": \"Nepali\", \"ori\": \"Odia\", \"pan\": \"Punjabi\", \"pol\": \"Polish\",\n",
        "    \"rus\": \"Russian\", \"spa\": \"Spanish\", \"swa\": \"Swahili\", \"tel\": \"Telugu\", \"tur\": \"Turkish\",\n",
        "    \"urd\": \"Urdu\", \"zho\": \"Chinese\"\n",
        "}\n",
        "\n",
        "# Team sharding\n",
        "N_PEOPLE = 5\n",
        "PERSON_ID = 1          # <-- each person sets 0..4\n",
        "\n",
        "# Two sessions per person:\n",
        "# Run the SAME script in 2 separate notebooks, each with a different API key:\n",
        "# - Notebook A: SESSION_ID=0\n",
        "# - Notebook B: SESSION_ID=1\n",
        "SESSION_ID = 0         # <-- set to 0 or 1 (or None if you don't want 2 sessions)\n",
        "N_SESSIONS_PER_PERSON = 2\n",
        "\n",
        "# Which languages THIS run should translate (non-teacher langs recommended)\n",
        "PERSON_LANGS = {\n",
        "  0: [\"urd\", \"hin\", \"hau\"],\n",
        "  1: [\"khm\", \"nep\", \"tel\"],\n",
        "  2: [\"ben\", \"amh\", \"ita\"],\n",
        "  3: [\"swa\", \"ori\", \"pan\"],\n",
        "  4: [\"zho\", \"mya\", \"fas\"],\n",
        "}\n",
        "LANGS_THIS_RUN = PERSON_LANGS[PERSON_ID]\n",
        "\n",
        "\n",
        "# Test mode\n",
        "TEST_MODE = False\n",
        "TEST_SAMPLES_PER_LANG = 5\n",
        "\n",
        "# OpenAI\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"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\")\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Batching + saving\n",
        "BATCH_SIZE = 30      # 20–50 good. Use 10 for debugging saving\n",
        "SAVE_EVERY = 10      # save every N batches. Use 1 for debugging saving\n",
        "\n",
        "# Retry\n",
        "MAX_RETRIES = 4\n",
        "BASE_BACKOFF = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) MARKER PROTECTION (same spirit as your old code)\n",
        "# =========================\n",
        "\n",
        "MARKER_PATTERNS = [\n",
        "    r'@\\w+',              # mentions\n",
        "    r'#\\w+',              # hashtags\n",
        "    r'https?://\\S+',      # URLs\n",
        "    r'www\\.\\S+',          # URLs\n",
        "]\n",
        "\n",
        "def extract_markers(text: str) -> set:\n",
        "    out = set()\n",
        "    for pat in MARKER_PATTERNS:\n",
        "        out.update(re.findall(pat, str(text)))\n",
        "    return out\n",
        "\n",
        "def stable_row_id(lang: str, text: str) -> str:\n",
        "    h = hashlib.sha256((str(lang) + \"||\" + str(text)).encode(\"utf-8\")).hexdigest()\n",
        "    return h[:16]\n",
        "\n",
        "def validate_translation(seed_text: str, translated: str) -> Tuple[bool, str]:\n",
        "    if not isinstance(translated, str) or len(translated.strip()) < 2:\n",
        "        return False, \"empty translation\"\n",
        "\n",
        "    # Marker constraint: translated must not introduce new markers\n",
        "    seed_markers = {m.lower() for m in extract_markers(seed_text)}\n",
        "    seed_markers.discard(\"@user\")  # allow @USER normalization\n",
        "    out_markers = {m.lower() for m in extract_markers(translated)}\n",
        "    out_markers.discard(\"@user\")\n",
        "\n",
        "    extra = out_markers - seed_markers\n",
        "    if extra:\n",
        "        return False, f\"introduced new marker(s): {list(extra)[:3]}\"\n",
        "\n",
        "    return True, \"ok\""
      ],
      "metadata": {
        "id": "MI2b-HFr1Mx-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) PROMPTS (high-quality, context-aware)\n",
        "# =========================\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a professional human translator and localization expert.\\n\"\n",
        "    \"Translate social media text into natural, fluent English while preserving meaning, tone, and pragmatic intent.\\n\"\n",
        "    \"Use culturally appropriate equivalents; do NOT translate word-by-word if it harms meaning.\\n\"\n",
        "    \"Do NOT add explanations. Return ONLY valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE_SINGLE = \"\"\"Translate the following social media post into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Preserve the original meaning, stance, and tone (including sarcasm, slang, dialect, irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor or soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Keep formatting minimal and readable.\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translation\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Batched version: returns list of translations aligned with inputs\n",
        "USER_TEMPLATE_BATCH = \"\"\"Translate the following social media posts into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "CRITICAL REQUIREMENTS (apply to every item):\n",
        "- Preserve meaning, stance, and tone (sarcasm/slang/dialect/irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor/soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Return EXACTLY one translation per input, same order.\n",
        "\n",
        "INPUTS (JSON array of strings):\n",
        "{inputs_json}\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translations\": [\"...\", \"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# 4) SHARDING (deterministic)\n",
        "# =========================\n",
        "\n",
        "def assign_person(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_PEOPLE\n",
        "\n",
        "def assign_session(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_SESSIONS_PER_PERSON\n",
        "\n",
        "def chunk_list(xs: List[Any], n: int) -> List[List[Any]]:\n",
        "    return [xs[i:i+n] for i in range(0, len(xs), n)]\n",
        "\n",
        "# =========================\n",
        "# 5) DURABLE SAVE (atomic + fsync)\n",
        "# =========================\n",
        "\n",
        "def flush_shard(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    pd.DataFrame(rows).to_csv(tmp, index=False)\n",
        "\n",
        "    # force flush to disk\n",
        "    with open(tmp, \"rb\") as f:\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# =========================\n",
        "# 6) OPENAI CALL (batch + retry + validation)\n",
        "# =========================\n",
        "\n",
        "def call_openai_translate_batch(\n",
        "    client: OpenAI,\n",
        "    lang_code: str,\n",
        "    seed_texts: List[str],\n",
        ") -> List[str]:\n",
        "    lang_name = LANG_MAP.get(lang_code, \"Unknown\")\n",
        "    user_prompt = USER_TEMPLATE_BATCH.format(\n",
        "        source_lang_name=lang_name,\n",
        "        lang_code=lang_code,\n",
        "        inputs_json=json.dumps(seed_texts, ensure_ascii=False)\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.2\n",
        "            )\n",
        "            data = json.loads(resp.choices[0].message.content)\n",
        "            outs = data.get(\"translations\", None)\n",
        "\n",
        "            if not isinstance(outs, list) or len(outs) != len(seed_texts):\n",
        "                raise ValueError(f\"Bad JSON: expected translations list of len {len(seed_texts)}\")\n",
        "\n",
        "            # Validate each translation (marker protection)\n",
        "            cleaned = []\n",
        "            for seed, tr in zip(seed_texts, outs):\n",
        "                ok, reason = validate_translation(seed, tr)\n",
        "                if not ok:\n",
        "                    raise ValueError(f\"Validation failed: {reason}\")\n",
        "                cleaned.append(str(tr).strip())\n",
        "\n",
        "            return cleaned\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == MAX_RETRIES:\n",
        "                break\n",
        "            time.sleep(BASE_BACKOFF * (2 ** (attempt - 1)) + 0.25 * attempt)\n",
        "\n",
        "    raise RuntimeError(f\"Translation failed after retries: {last_err}\")\n",
        "\n",
        "def safe_read_csv(path: str) -> pd.DataFrame:\n",
        "    if (not os.path.exists(path)) or os.path.getsize(path) < 5:\n",
        "        # empty or non-existent -> treat as no previous progress\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path)\n",
        "# =========================\n",
        "# 7) MAIN\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # 1) Load\n",
        "    df = pd.read_csv(MASTER_IN)\n",
        "\n",
        "    # 2) Ensure columns\n",
        "    if OUT_COL not in df.columns:\n",
        "        df[OUT_COL] = pd.NA\n",
        "\n",
        "    if \"row_id\" not in df.columns:\n",
        "        df[\"row_id\"] = [stable_row_id(l, t) for l, t in zip(df[LANG_COL], df[TEXT_COL])]\n",
        "\n",
        "    # 3) Copy-only teacher langs (NO API)\n",
        "    teacher_mask = df[LANG_COL].isin(TEACHER_LANGS)\n",
        "    df.loc[teacher_mask & df[OUT_COL].isna(), OUT_COL] = df.loc[teacher_mask & df[OUT_COL].isna(), TEXT_COL]\n",
        "\n",
        "    # 4) Work mask: chosen splits + langs + missing + not teacher\n",
        "    split_mask = df[SPLIT_COL].astype(str).isin(TRANSLATE_SPLITS)\n",
        "    lang_mask = df[LANG_COL].isin(LANGS_THIS_RUN)\n",
        "    need_mask = split_mask & lang_mask & (~teacher_mask) & (df[OUT_COL].isna())\n",
        "\n",
        "    # 5) Apply team sharding (ONLY if PERSON_ID / SESSION_ID are set)\n",
        "    idx = df.index[need_mask].tolist()\n",
        "\n",
        "    if SESSION_ID is not None:\n",
        "        idx = [i for i in idx if assign_session(str(df.at[i, \"row_id\"])) == SESSION_ID]\n",
        "\n",
        "\n",
        "    # 6) TEST_MODE sampling\n",
        "    if TEST_MODE:\n",
        "        sampled = []\n",
        "        for lg in LANGS_THIS_RUN:\n",
        "            candidates = [i for i in idx if df.at[i, LANG_COL] == lg]\n",
        "            sampled.extend(candidates[:TEST_SAMPLES_PER_LANG])\n",
        "        idx = sampled\n",
        "\n",
        "    # 7) Output path for shard (safe even when PERSON_ID/SESSION_ID is None)\n",
        "    person_tag = f\"person{PERSON_ID}\" if PERSON_ID is not None else \"personALL\"\n",
        "    sess_tag   = f\"S{SESSION_ID}\"     if SESSION_ID is not None else \"SALL\"\n",
        "\n",
        "    out_path = os.path.join(\n",
        "        OUT_DIR,\n",
        "        f\"shard_{person_tag}_{sess_tag}_\" + \"_\".join(LANGS_THIS_RUN) + \".csv\"\n",
        "    )\n",
        "    print(\"Shard output path:\", out_path)\n",
        "\n",
        "\n",
        "    # 8) Resume support (robust to empty/partial files)\n",
        "    done_row_ids = set()\n",
        "    shard_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    prev = safe_read_csv(out_path)\n",
        "    if not prev.empty:\n",
        "        # only accept if it actually has the expected columns\n",
        "        if \"row_id\" in prev.columns and OUT_COL in prev.columns:\n",
        "            done_row_ids = set(prev.loc[prev[OUT_COL].notna(), \"row_id\"].astype(str).tolist())\n",
        "        shard_rows = prev.to_dict(\"records\")\n",
        "        print(\"Resume: already translated in shard:\", len(done_row_ids))\n",
        "    else:\n",
        "        print(\"Resume: no valid previous shard (file missing or empty). Starting fresh.\")\n",
        "\n",
        "    idx = [i for i in idx if str(df.at[i, \"row_id\"]) not in done_row_ids]\n",
        "    print(\"Total master rows:\", len(df))\n",
        "    print(\"Teacher rows (copy-only) missing text_en:\", int((teacher_mask & df[OUT_COL].isna()).sum()))\n",
        "    print(\"Rows assigned to THIS run (after resume):\", len(idx))\n",
        "\n",
        "    if len(idx) == 0:\n",
        "        # still save shard (might contain previous work)\n",
        "        if shard_rows:\n",
        "            flush_shard(shard_rows, out_path)\n",
        "            print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "        else:\n",
        "            print(\"[FINAL SAVE] skipped (no rows)\")\n",
        "\n",
        "\n",
        "    # 9) Group by language (batch per language)\n",
        "    groups: Dict[str, List[int]] = {}\n",
        "    for i in idx:\n",
        "        lg = str(df.at[i, LANG_COL])\n",
        "        groups.setdefault(lg, []).append(i)\n",
        "\n",
        "    # 10) Client\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # 11) Translate\n",
        "    total = sum(len(v) for v in groups.values())\n",
        "    pbar = tqdm(total=total, desc=\"Translating\")\n",
        "    batches_since_save = 0\n",
        "\n",
        "    failures = []\n",
        "    try:\n",
        "        for lg, rows in groups.items():\n",
        "            for chunk in chunk_list(rows, BATCH_SIZE):\n",
        "                seed_texts = [str(df.at[i, TEXT_COL]) for i in chunk]\n",
        "\n",
        "                try:\n",
        "                    outs = call_openai_translate_batch(client, lg, seed_texts)\n",
        "                except Exception as e:\n",
        "                    # fall back to single-item calls (debug + salvage)\n",
        "                    outs = []\n",
        "                    for one_i, one_seed in zip(chunk, seed_texts):\n",
        "                        try:\n",
        "                            # single prompt fallback\n",
        "                            lang_name = LANG_MAP.get(lg, \"Unknown\")\n",
        "                            user_prompt = USER_TEMPLATE_SINGLE.format(\n",
        "                                source_lang_name=lang_name,\n",
        "                                lang_code=lg,\n",
        "                                text=one_seed\n",
        "                            )\n",
        "                            resp = client.chat.completions.create(\n",
        "                                model=MODEL,\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                                ],\n",
        "                                response_format={\"type\": \"json_object\"},\n",
        "                                temperature=0.2\n",
        "                            )\n",
        "                            data = json.loads(resp.choices[0].message.content)\n",
        "                            tr = data.get(\"translation\", \"\")\n",
        "                            ok, reason = validate_translation(one_seed, tr)\n",
        "                            if not ok:\n",
        "                                raise ValueError(reason)\n",
        "                            outs.append(str(tr).strip())\n",
        "                        except Exception as e2:\n",
        "                            failures.append({\n",
        "                                \"row_id\": str(df.at[one_i, \"row_id\"]),\n",
        "                                \"lang\": lg,\n",
        "                                \"error\": str(e2),\n",
        "                                \"seed_text\": one_seed[:500]\n",
        "                            })\n",
        "                            outs.append(None)\n",
        "\n",
        "                # write successes\n",
        "                for i, tr in zip(chunk, outs):\n",
        "                    if tr is None:\n",
        "                        continue\n",
        "                    df.at[i, OUT_COL] = tr\n",
        "                    shard_rows.append({\n",
        "                        \"row_id\": str(df.at[i, \"row_id\"]),\n",
        "                        LANG_COL: str(df.at[i, LANG_COL]),\n",
        "                        SPLIT_COL: str(df.at[i, SPLIT_COL]),\n",
        "                        TEXT_COL: str(df.at[i, TEXT_COL]),\n",
        "                        OUT_COL: tr\n",
        "                    })\n",
        "\n",
        "                pbar.update(len(chunk))\n",
        "                batches_since_save += 1\n",
        "\n",
        "                if batches_since_save >= SAVE_EVERY:\n",
        "                    if shard_rows:\n",
        "                        flush_shard(shard_rows, out_path)\n",
        "                        print(f\"[SAVED] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "                    else:\n",
        "                        print(\"[SAVED] skipped (no rows yet)\")\n",
        "                    batches_since_save = 0\n",
        "\n",
        "\n",
        "        # final flush\n",
        "        flush_shard(shard_rows, out_path)\n",
        "        print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "\n",
        "        # failures log\n",
        "        if failures:\n",
        "            fail_path = out_path.replace(\".csv\", \"_failures.jsonl\")\n",
        "            with open(fail_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for r in failures:\n",
        "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "            print(\"Failures logged to:\", fail_path, \"count=\", len(failures))\n",
        "\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "    # TEST_MODE preview\n",
        "    if TEST_MODE:\n",
        "        print(\"\\n=== TEST MODE PREVIEW ===\")\n",
        "        if shard_rows:\n",
        "            preview = pd.DataFrame(shard_rows).tail(10)\n",
        "            print(preview[[LANG_COL, TEXT_COL, OUT_COL]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"(empty) No successful translations generated.\")\n",
        "\n",
        "    print(\"DONE. Shard saved:\", out_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTXKbfW51Wit",
        "outputId": "899f2652-f8ff-4ab4-c514-dc0e3980d38f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shard output path: /content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel.csv\n",
            "Resume: no valid previous shard (file missing or empty). Starting fresh.\n",
            "Total master rows: 77368\n",
            "Teacher rows (copy-only) missing text_en: 0\n",
            "Rows assigned to THIS run (after resume): 5481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:   5%|▌         | 300/5481 [03:51<59:41,  1.45it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=300 bytes=200377 mtime=Sun Jan  4 14:31:38 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  11%|█         | 600/5481 [07:37<59:23,  1.37it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=600 bytes=395847 mtime=Sun Jan  4 14:35:24 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  16%|█▋        | 900/5481 [11:16<51:52,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=900 bytes=594174 mtime=Sun Jan  4 14:39:03 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  22%|██▏       | 1200/5481 [14:55<53:09,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1200 bytes=786319 mtime=Sun Jan  4 14:42:43 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  27%|██▋       | 1500/5481 [18:13<44:44,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1500 bytes=986075 mtime=Sun Jan  4 14:46:00 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  33%|███▎      | 1800/5481 [23:05<49:39,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1799 bytes=1173971 mtime=Sun Jan  4 14:50:52 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  38%|███▊      | 2100/5481 [27:37<44:45,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2099 bytes=1374387 mtime=Sun Jan  4 14:55:24 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  44%|████▍     | 2400/5481 [32:47<41:48,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2398 bytes=1567678 mtime=Sun Jan  4 15:00:34 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  49%|████▉     | 2700/5481 [36:33<36:26,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2698 bytes=1769002 mtime=Sun Jan  4 15:04:21 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  55%|█████▍    | 3000/5481 [40:32<32:16,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2998 bytes=1964729 mtime=Sun Jan  4 15:08:19 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  60%|█████▉    | 3275/5481 [43:54<25:22,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3273 bytes=2138138 mtime=Sun Jan  4 15:11:41 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  65%|██████▌   | 3575/5481 [47:22<22:06,  1.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3572 bytes=2271393 mtime=Sun Jan  4 15:15:09 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  71%|███████   | 3875/5481 [49:15<08:57,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3872 bytes=2395051 mtime=Sun Jan  4 15:17:02 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  76%|███████▌  | 4175/5481 [56:41<35:39,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4114 bytes=2472330 mtime=Sun Jan  4 15:24:28 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  81%|████████  | 4446/5481 [1:03:04<39:17,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4354 bytes=2575609 mtime=Sun Jan  4 15:30:51 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  87%|████████▋ | 4746/5481 [1:05:51<07:04,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4654 bytes=2704171 mtime=Sun Jan  4 15:33:38 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  92%|█████████▏| 5046/5481 [1:08:46<04:03,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4954 bytes=2831082 mtime=Sun Jan  4 15:36:33 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  98%|█████████▊| 5346/5481 [1:11:34<01:06,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=5254 bytes=2963145 mtime=Sun Jan  4 15:39:22 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 5481/5481 [1:12:46<00:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL SAVE] rows=5389 bytes=3022416 mtime=Sun Jan  4 15:40:33 2026\n",
            "Failures logged to: /content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel_failures.jsonl count= 92\n",
            "DONE. Shard saved: /content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, hashlib, re, time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# ====== PATHS ======\n",
        "MASTER = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "FAILURES = \"/content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel_failures.jsonl\"\n",
        "OUT = \"/content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel_retry.csv\"\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "MODEL = \"gpt-4o-mini\"   # use the same model you used before\n",
        "OPENAI_API_KEY = \"Hidden\"\n",
        "\n",
        "LANG_COL = \"lang\"\n",
        "TEXT_COL = \"text\"\n",
        "SPLIT_COL = \"split\"\n",
        "OUT_COL = \"text_en\"\n",
        "\n",
        "LANG_MAP = {\n",
        "  \"khm\":\"Khmer\",\"nep\":\"Nepali\",\"tel\":\"Telugu\",\"swa\":\"Swahili\",\"ita\":\"Italian\",\n",
        "  \"zho\":\"Chinese\",\"hau\":\"Hausa\",\"urd\":\"Urdu\",\"ben\":\"Bengali\",\"amh\":\"Amharic\",\n",
        "  \"fas\":\"Persian\",\"mya\":\"Burmese\",\"ori\":\"Odia\",\"pan\":\"Punjabi\",\n",
        "  \"eng\":\"English\",\"spa\":\"Spanish\",\"deu\":\"German\",\"rus\":\"Russian\",\"tur\":\"Turkish\",\"pol\":\"Polish\",\"arb\":\"Arabic\"\n",
        "}\n",
        "\n",
        "# ====== MARKER PROTECTION (same as your working code) ======\n",
        "MARKER_PATTERNS = [r'@\\w+', r'#\\w+', r'https?://\\S+', r'www\\.\\S+']\n",
        "\n",
        "def extract_markers(text: str) -> set:\n",
        "    out = set()\n",
        "    for pat in MARKER_PATTERNS:\n",
        "        out.update(re.findall(pat, text))\n",
        "    return out\n",
        "\n",
        "def validate_translation(seed_text: str, translated: str):\n",
        "    if not isinstance(translated, str) or len(translated.strip()) < 2:\n",
        "        return False, \"empty translation\"\n",
        "    seed_markers = {m.lower() for m in extract_markers(seed_text)}\n",
        "    seed_markers.discard(\"@user\")\n",
        "    out_markers = {m.lower() for m in extract_markers(translated)}\n",
        "    out_markers.discard(\"@user\")\n",
        "    extra = out_markers - seed_markers\n",
        "    if extra:\n",
        "        return False, f\"introduced new marker(s): {list(extra)[:3]}\"\n",
        "    return True, \"ok\"\n",
        "\n",
        "def stable_row_id(lang: str, text: str) -> str:\n",
        "    h = hashlib.sha256((str(lang)+\"||\"+str(text)).encode(\"utf-8\")).hexdigest()\n",
        "    return h[:16]\n",
        "\n",
        "# ====== PROMPTS ======\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a professional human translator and localization expert.\\n\"\n",
        "    \"Translate social media text into natural, fluent English while preserving meaning, tone, and pragmatic intent.\\n\"\n",
        "    \"Use culturally appropriate equivalents; do NOT translate word-by-word if it harms meaning.\\n\"\n",
        "    \"Do NOT add explanations. Return ONLY valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE_SINGLE = \"\"\"Translate the following social media post into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Preserve the original meaning, stance, and tone (including sarcasm, slang, dialect, irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor or soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Keep formatting minimal and readable.\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translation\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# ====== RETRY ======\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "df = pd.read_csv(MASTER)\n",
        "\n",
        "# ensure row_id exists (THIS FIXES YOUR ERROR)\n",
        "if \"row_id\" not in df.columns:\n",
        "    df[\"row_id\"] = [stable_row_id(l, t) for l, t in zip(df[LANG_COL], df[TEXT_COL])]\n",
        "\n",
        "# load failed row_ids\n",
        "failed_rows = []\n",
        "with open(FAILURES, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        failed_rows.append(json.loads(line))\n",
        "\n",
        "failed_ids = {r[\"row_id\"] for r in failed_rows}\n",
        "print(\"Retrying rows:\", len(failed_ids))\n",
        "\n",
        "retry_df = df[df[\"row_id\"].astype(str).isin(failed_ids)].copy()\n",
        "print(\"Found in master:\", len(retry_df))\n",
        "\n",
        "out_rows = []\n",
        "still_failed = []\n",
        "\n",
        "for _, row in tqdm(retry_df.iterrows(), total=len(retry_df), desc=\"Retry\"):\n",
        "    seed = str(row[TEXT_COL])\n",
        "    lg = str(row[LANG_COL])\n",
        "    try:\n",
        "        prompt = USER_TEMPLATE_SINGLE.format(\n",
        "            source_lang_name=LANG_MAP.get(lg, \"Unknown\"),\n",
        "            lang_code=lg,\n",
        "            text=seed\n",
        "        )\n",
        "        resp = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "            temperature=0.2\n",
        "        )\n",
        "        tr = json.loads(resp.choices[0].message.content).get(\"translation\", \"\")\n",
        "        ok, reason = validate_translation(seed, tr)\n",
        "        if not ok:\n",
        "            raise ValueError(reason)\n",
        "\n",
        "        out_rows.append({\n",
        "            \"row_id\": row[\"row_id\"],\n",
        "            \"lang\": lg,\n",
        "            \"split\": row.get(SPLIT_COL, \"\"),\n",
        "            \"text\": seed,\n",
        "            \"text_en\": tr.strip()\n",
        "        })\n",
        "    except Exception as e:\n",
        "        still_failed.append({\n",
        "            \"row_id\": row[\"row_id\"],\n",
        "            \"lang\": lg,\n",
        "            \"error\": str(e),\n",
        "            \"seed_text\": seed[:500]\n",
        "        })\n",
        "\n",
        "pd.DataFrame(out_rows).to_csv(OUT, index=False)\n",
        "print(\"Retry saved:\", OUT, \"rows:\", len(out_rows))\n",
        "\n",
        "if still_failed:\n",
        "    out_fail = OUT.replace(\".csv\", \"_still_failed.jsonl\")\n",
        "    with open(out_fail, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in still_failed:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "    print(\"Still-failed logged:\", out_fail, \"count:\", len(still_failed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v3PPyfh2uyy",
        "outputId": "26eee709-5503-4846-925d-7c73345454b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrying rows: 92\n",
            "Found in master: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retry: 100%|██████████| 92/92 [21:28<00:00, 14.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retry saved: /content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel_retry.csv rows: 2\n",
            "Still-failed logged: /content/drive/MyDrive/translation_shards/shard_person1_S0_khm_nep_tel_retry_still_failed.jsonl count: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}