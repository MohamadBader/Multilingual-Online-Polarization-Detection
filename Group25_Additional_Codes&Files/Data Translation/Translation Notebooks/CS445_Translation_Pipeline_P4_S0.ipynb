{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIUvSFU524gp",
        "outputId": "f71be2fd-4d7c-484d-a6b9-a7b98be9efc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kBLw2xlK09o3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL TEAM TRANSLATION SCRIPT (DRIVE-SAFE + NO UNNEEDED API)\n",
        "# - Copy-only for TEACHER_LANGS (NO API calls)\n",
        "# - Translate only selected languages for THIS run\n",
        "# - Team sharding: 5 people + optional 2 sessions/person (2 notebooks/keys)\n",
        "# - High-quality prompts (similar to your old logic)\n",
        "# - Marker protection + validation (no new @/#/URLs)\n",
        "# - Batched translation (faster)\n",
        "# - DURABLE checkpointing to Google Drive (atomic write + fsync)\n",
        "# - Resume-safe (skips already translated row_ids in shard file)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# 1) CONFIG\n",
        "# =========================\n",
        "\n",
        "# Input master (original)\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "\n",
        "# Output directory for shards (each run writes its own file)\n",
        "OUT_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Output column name\n",
        "OUT_COL = \"text_en\"\n",
        "\n",
        "# Master columns\n",
        "TEXT_COL = \"text\"\n",
        "LANG_COL = \"lang\"\n",
        "SPLIT_COL = \"split\"\n",
        "\n",
        "# Translate only train by default (recommended)\n",
        "TRANSLATE_SPLITS = {\"train\"}  # change to {\"train\",\"dev\"} if you really want dev too\n",
        "\n",
        "# Teacher languages: copy-only (no API calls)\n",
        "TEACHER_LANGS = {\"eng\", \"spa\", \"deu\", \"rus\", \"tur\", \"pol\", \"arb\"}\n",
        "\n",
        "# Language names (for prompt)\n",
        "LANG_MAP = {\n",
        "    \"amh\": \"Amharic\", \"arb\": \"Arabic\", \"ben\": \"Bengali\", \"deu\": \"German\", \"eng\": \"English\",\n",
        "    \"fas\": \"Persian\", \"hau\": \"Hausa\", \"hin\": \"Hindi\", \"ita\": \"Italian\", \"khm\": \"Khmer\",\n",
        "    \"mya\": \"Burmese\", \"nep\": \"Nepali\", \"ori\": \"Odia\", \"pan\": \"Punjabi\", \"pol\": \"Polish\",\n",
        "    \"rus\": \"Russian\", \"spa\": \"Spanish\", \"swa\": \"Swahili\", \"tel\": \"Telugu\", \"tur\": \"Turkish\",\n",
        "    \"urd\": \"Urdu\", \"zho\": \"Chinese\"\n",
        "}\n",
        "\n",
        "# Team sharding\n",
        "N_PEOPLE = 5\n",
        "PERSON_ID = 4          # <-- each person sets 0..4\n",
        "\n",
        "# Two sessions per person:\n",
        "# Run the SAME script in 2 separate notebooks, each with a different API key:\n",
        "# - Notebook A: SESSION_ID=0\n",
        "# - Notebook B: SESSION_ID=1\n",
        "SESSION_ID = 0         # <-- set to 0 or 1 (or None if you don't want 2 sessions)\n",
        "N_SESSIONS_PER_PERSON = 2\n",
        "\n",
        "# Which languages THIS run should translate (non-teacher langs recommended)\n",
        "PERSON_LANGS = {\n",
        "  0: [\"urd\", \"hin\", \"hau\"],\n",
        "  1: [\"khm\", \"nep\", \"tel\"],\n",
        "  2: [\"ben\", \"amh\", \"ita\"],\n",
        "  3: [\"swa\", \"ori\", \"pan\"],\n",
        "  4: [\"zho\", \"mya\", \"fas\"],\n",
        "}\n",
        "LANGS_THIS_RUN = PERSON_LANGS[PERSON_ID]\n",
        "\n",
        "\n",
        "# Test mode\n",
        "TEST_MODE = False\n",
        "TEST_SAMPLES_PER_LANG = 5\n",
        "\n",
        "# OpenAI\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"PRIVATE\")\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Batching + saving\n",
        "BATCH_SIZE = 30      # 20–50 good. Use 10 for debugging saving\n",
        "SAVE_EVERY = 10      # save every N batches. Use 1 for debugging saving\n",
        "\n",
        "# Retry\n",
        "MAX_RETRIES = 4\n",
        "BASE_BACKOFF = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) MARKER PROTECTION (same spirit as your old code)\n",
        "# =========================\n",
        "\n",
        "MARKER_PATTERNS = [\n",
        "    r'@\\w+',              # mentions\n",
        "    r'#\\w+',              # hashtags\n",
        "    r'https?://\\S+',      # URLs\n",
        "    r'www\\.\\S+',          # URLs\n",
        "]\n",
        "\n",
        "def extract_markers(text: str) -> set:\n",
        "    out = set()\n",
        "    for pat in MARKER_PATTERNS:\n",
        "        out.update(re.findall(pat, str(text)))\n",
        "    return out\n",
        "\n",
        "def stable_row_id(lang: str, text: str) -> str:\n",
        "    h = hashlib.sha256((str(lang) + \"||\" + str(text)).encode(\"utf-8\")).hexdigest()\n",
        "    return h[:16]\n",
        "\n",
        "def validate_translation(seed_text: str, translated: str) -> Tuple[bool, str]:\n",
        "    if not isinstance(translated, str) or len(translated.strip()) < 2:\n",
        "        return False, \"empty translation\"\n",
        "\n",
        "    # Marker constraint: translated must not introduce new markers\n",
        "    seed_markers = {m.lower() for m in extract_markers(seed_text)}\n",
        "    seed_markers.discard(\"@user\")  # allow @USER normalization\n",
        "    out_markers = {m.lower() for m in extract_markers(translated)}\n",
        "    out_markers.discard(\"@user\")\n",
        "\n",
        "    extra = out_markers - seed_markers\n",
        "    if extra:\n",
        "        return False, f\"introduced new marker(s): {list(extra)[:3]}\"\n",
        "\n",
        "    return True, \"ok\""
      ],
      "metadata": {
        "id": "MI2b-HFr1Mx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) PROMPTS (high-quality, context-aware)\n",
        "# =========================\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a professional human translator and localization expert.\\n\"\n",
        "    \"Translate social media text into natural, fluent English while preserving meaning, tone, and pragmatic intent.\\n\"\n",
        "    \"Use culturally appropriate equivalents; do NOT translate word-by-word if it harms meaning.\\n\"\n",
        "    \"Do NOT add explanations. Return ONLY valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE_SINGLE = \"\"\"Translate the following social media post into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Preserve the original meaning, stance, and tone (including sarcasm, slang, dialect, irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor or soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Keep formatting minimal and readable.\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translation\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Batched version: returns list of translations aligned with inputs\n",
        "USER_TEMPLATE_BATCH = \"\"\"Translate the following social media posts into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "CRITICAL REQUIREMENTS (apply to every item):\n",
        "- Preserve meaning, stance, and tone (sarcasm/slang/dialect/irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor/soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Return EXACTLY one translation per input, same order.\n",
        "\n",
        "INPUTS (JSON array of strings):\n",
        "{inputs_json}\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translations\": [\"...\", \"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# 4) SHARDING (deterministic)\n",
        "# =========================\n",
        "\n",
        "def assign_person(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_PEOPLE\n",
        "\n",
        "def assign_session(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_SESSIONS_PER_PERSON\n",
        "\n",
        "def chunk_list(xs: List[Any], n: int) -> List[List[Any]]:\n",
        "    return [xs[i:i+n] for i in range(0, len(xs), n)]\n",
        "\n",
        "# =========================\n",
        "# 5) DURABLE SAVE (atomic + fsync)\n",
        "# =========================\n",
        "\n",
        "def flush_shard(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    pd.DataFrame(rows).to_csv(tmp, index=False)\n",
        "\n",
        "    # force flush to disk\n",
        "    with open(tmp, \"rb\") as f:\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# =========================\n",
        "# 6) OPENAI CALL (batch + retry + validation)\n",
        "# =========================\n",
        "\n",
        "def call_openai_translate_batch(\n",
        "    client: OpenAI,\n",
        "    lang_code: str,\n",
        "    seed_texts: List[str],\n",
        ") -> List[str]:\n",
        "    lang_name = LANG_MAP.get(lang_code, \"Unknown\")\n",
        "    user_prompt = USER_TEMPLATE_BATCH.format(\n",
        "        source_lang_name=lang_name,\n",
        "        lang_code=lang_code,\n",
        "        inputs_json=json.dumps(seed_texts, ensure_ascii=False)\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.2\n",
        "            )\n",
        "            data = json.loads(resp.choices[0].message.content)\n",
        "            outs = data.get(\"translations\", None)\n",
        "\n",
        "            if not isinstance(outs, list) or len(outs) != len(seed_texts):\n",
        "                raise ValueError(f\"Bad JSON: expected translations list of len {len(seed_texts)}\")\n",
        "\n",
        "            # Validate each translation (marker protection)\n",
        "            cleaned = []\n",
        "            for seed, tr in zip(seed_texts, outs):\n",
        "                ok, reason = validate_translation(seed, tr)\n",
        "                if not ok:\n",
        "                    raise ValueError(f\"Validation failed: {reason}\")\n",
        "                cleaned.append(str(tr).strip())\n",
        "\n",
        "            return cleaned\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == MAX_RETRIES:\n",
        "                break\n",
        "            time.sleep(BASE_BACKOFF * (2 ** (attempt - 1)) + 0.25 * attempt)\n",
        "\n",
        "    raise RuntimeError(f\"Translation failed after retries: {last_err}\")\n",
        "\n",
        "def safe_read_csv(path: str) -> pd.DataFrame:\n",
        "    if (not os.path.exists(path)) or os.path.getsize(path) < 5:\n",
        "        # empty or non-existent -> treat as no previous progress\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path)\n",
        "# =========================\n",
        "# 7) MAIN\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # 1) Load\n",
        "    df = pd.read_csv(MASTER_IN)\n",
        "\n",
        "    # 2) Ensure columns\n",
        "    if OUT_COL not in df.columns:\n",
        "        df[OUT_COL] = pd.NA\n",
        "\n",
        "    if \"row_id\" not in df.columns:\n",
        "        df[\"row_id\"] = [stable_row_id(l, t) for l, t in zip(df[LANG_COL], df[TEXT_COL])]\n",
        "\n",
        "    # 3) Copy-only teacher langs (NO API)\n",
        "    teacher_mask = df[LANG_COL].isin(TEACHER_LANGS)\n",
        "    df.loc[teacher_mask & df[OUT_COL].isna(), OUT_COL] = df.loc[teacher_mask & df[OUT_COL].isna(), TEXT_COL]\n",
        "\n",
        "    # 4) Work mask: chosen splits + langs + missing + not teacher\n",
        "    split_mask = df[SPLIT_COL].astype(str).isin(TRANSLATE_SPLITS)\n",
        "    lang_mask = df[LANG_COL].isin(LANGS_THIS_RUN)\n",
        "    need_mask = split_mask & lang_mask & (~teacher_mask) & (df[OUT_COL].isna())\n",
        "\n",
        "    # 5) Apply team sharding (ONLY if PERSON_ID / SESSION_ID are set)\n",
        "    idx = df.index[need_mask].tolist()\n",
        "\n",
        "    if SESSION_ID is not None:\n",
        "        idx = [i for i in idx if assign_session(str(df.at[i, \"row_id\"])) == SESSION_ID]\n",
        "\n",
        "\n",
        "    # 6) TEST_MODE sampling\n",
        "    if TEST_MODE:\n",
        "        sampled = []\n",
        "        for lg in LANGS_THIS_RUN:\n",
        "            candidates = [i for i in idx if df.at[i, LANG_COL] == lg]\n",
        "            sampled.extend(candidates[:TEST_SAMPLES_PER_LANG])\n",
        "        idx = sampled\n",
        "\n",
        "    # 7) Output path for shard (safe even when PERSON_ID/SESSION_ID is None)\n",
        "    person_tag = f\"person{PERSON_ID}\" if PERSON_ID is not None else \"personALL\"\n",
        "    sess_tag   = f\"S{SESSION_ID}\"     if SESSION_ID is not None else \"SALL\"\n",
        "\n",
        "    out_path = os.path.join(\n",
        "        OUT_DIR,\n",
        "        f\"shard_{person_tag}_{sess_tag}_\" + \"_\".join(LANGS_THIS_RUN) + \".csv\"\n",
        "    )\n",
        "    print(\"Shard output path:\", out_path)\n",
        "\n",
        "\n",
        "    # 8) Resume support (robust to empty/partial files)\n",
        "    done_row_ids = set()\n",
        "    shard_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    prev = safe_read_csv(out_path)\n",
        "    if not prev.empty:\n",
        "        # only accept if it actually has the expected columns\n",
        "        if \"row_id\" in prev.columns and OUT_COL in prev.columns:\n",
        "            done_row_ids = set(prev.loc[prev[OUT_COL].notna(), \"row_id\"].astype(str).tolist())\n",
        "        shard_rows = prev.to_dict(\"records\")\n",
        "        print(\"Resume: already translated in shard:\", len(done_row_ids))\n",
        "    else:\n",
        "        print(\"Resume: no valid previous shard (file missing or empty). Starting fresh.\")\n",
        "\n",
        "    idx = [i for i in idx if str(df.at[i, \"row_id\"]) not in done_row_ids]\n",
        "    print(\"Total master rows:\", len(df))\n",
        "    print(\"Teacher rows (copy-only) missing text_en:\", int((teacher_mask & df[OUT_COL].isna()).sum()))\n",
        "    print(\"Rows assigned to THIS run (after resume):\", len(idx))\n",
        "\n",
        "    if len(idx) == 0:\n",
        "        # still save shard (might contain previous work)\n",
        "        if shard_rows:\n",
        "            flush_shard(shard_rows, out_path)\n",
        "            print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "        else:\n",
        "            print(\"[FINAL SAVE] skipped (no rows)\")\n",
        "\n",
        "\n",
        "    # 9) Group by language (batch per language)\n",
        "    groups: Dict[str, List[int]] = {}\n",
        "    for i in idx:\n",
        "        lg = str(df.at[i, LANG_COL])\n",
        "        groups.setdefault(lg, []).append(i)\n",
        "\n",
        "    # 10) Client\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # 11) Translate\n",
        "    total = sum(len(v) for v in groups.values())\n",
        "    pbar = tqdm(total=total, desc=\"Translating\")\n",
        "    batches_since_save = 0\n",
        "\n",
        "    failures = []\n",
        "    try:\n",
        "        for lg, rows in groups.items():\n",
        "            for chunk in chunk_list(rows, BATCH_SIZE):\n",
        "                seed_texts = [str(df.at[i, TEXT_COL]) for i in chunk]\n",
        "\n",
        "                try:\n",
        "                    outs = call_openai_translate_batch(client, lg, seed_texts)\n",
        "                except Exception as e:\n",
        "                    # fall back to single-item calls (debug + salvage)\n",
        "                    outs = []\n",
        "                    for one_i, one_seed in zip(chunk, seed_texts):\n",
        "                        try:\n",
        "                            # single prompt fallback\n",
        "                            lang_name = LANG_MAP.get(lg, \"Unknown\")\n",
        "                            user_prompt = USER_TEMPLATE_SINGLE.format(\n",
        "                                source_lang_name=lang_name,\n",
        "                                lang_code=lg,\n",
        "                                text=one_seed\n",
        "                            )\n",
        "                            resp = client.chat.completions.create(\n",
        "                                model=MODEL,\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                                ],\n",
        "                                response_format={\"type\": \"json_object\"},\n",
        "                                temperature=0.2\n",
        "                            )\n",
        "                            data = json.loads(resp.choices[0].message.content)\n",
        "                            tr = data.get(\"translation\", \"\")\n",
        "                            ok, reason = validate_translation(one_seed, tr)\n",
        "                            if not ok:\n",
        "                                raise ValueError(reason)\n",
        "                            outs.append(str(tr).strip())\n",
        "                        except Exception as e2:\n",
        "                            failures.append({\n",
        "                                \"row_id\": str(df.at[one_i, \"row_id\"]),\n",
        "                                \"lang\": lg,\n",
        "                                \"error\": str(e2),\n",
        "                                \"seed_text\": one_seed[:500]\n",
        "                            })\n",
        "                            outs.append(None)\n",
        "\n",
        "                # write successes\n",
        "                for i, tr in zip(chunk, outs):\n",
        "                    if tr is None:\n",
        "                        continue\n",
        "                    df.at[i, OUT_COL] = tr\n",
        "                    shard_rows.append({\n",
        "                        \"row_id\": str(df.at[i, \"row_id\"]),\n",
        "                        LANG_COL: str(df.at[i, LANG_COL]),\n",
        "                        SPLIT_COL: str(df.at[i, SPLIT_COL]),\n",
        "                        TEXT_COL: str(df.at[i, TEXT_COL]),\n",
        "                        OUT_COL: tr\n",
        "                    })\n",
        "\n",
        "                pbar.update(len(chunk))\n",
        "                batches_since_save += 1\n",
        "\n",
        "                if batches_since_save >= SAVE_EVERY:\n",
        "                    if shard_rows:\n",
        "                        flush_shard(shard_rows, out_path)\n",
        "                        print(f\"[SAVED] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "                    else:\n",
        "                        print(\"[SAVED] skipped (no rows yet)\")\n",
        "                    batches_since_save = 0\n",
        "\n",
        "\n",
        "        # final flush\n",
        "        flush_shard(shard_rows, out_path)\n",
        "        print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "\n",
        "        # failures log\n",
        "        if failures:\n",
        "            fail_path = out_path.replace(\".csv\", \"_failures.jsonl\")\n",
        "            with open(fail_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for r in failures:\n",
        "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "            print(\"Failures logged to:\", fail_path, \"count=\", len(failures))\n",
        "\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "    # TEST_MODE preview\n",
        "    if TEST_MODE:\n",
        "        print(\"\\n=== TEST MODE PREVIEW ===\")\n",
        "        if shard_rows:\n",
        "            preview = pd.DataFrame(shard_rows).tail(10)\n",
        "            print(preview[[LANG_COL, TEXT_COL, OUT_COL]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"(empty) No successful translations generated.\")\n",
        "\n",
        "    print(\"DONE. Shard saved:\", out_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTXKbfW51Wit",
        "outputId": "4853b0c1-893b-43f3-db99-fc71bc43ec4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shard output path: /content/drive/MyDrive/translation_shards/shard_person4_S0_zho_mya_fas.csv\n",
            "Resume: no valid previous shard (file missing or empty). Starting fresh.\n",
            "Total master rows: 77368\n",
            "Teacher rows (copy-only) missing text_en: 0\n",
            "Rows assigned to THIS run (after resume): 5189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:   6%|▌         | 300/5189 [03:01<45:56,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=300 bytes=80408 mtime=Sun Jan  4 15:21:08 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  12%|█▏        | 600/5189 [05:41<41:55,  1.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=600 bytes=162091 mtime=Sun Jan  4 15:23:48 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  17%|█▋        | 900/5189 [08:19<37:50,  1.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=900 bytes=243702 mtime=Sun Jan  4 15:26:26 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  23%|██▎       | 1200/5189 [11:02<35:31,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1200 bytes=320577 mtime=Sun Jan  4 15:29:09 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  29%|██▉       | 1500/5189 [14:08<36:20,  1.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1500 bytes=398163 mtime=Sun Jan  4 15:32:15 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  35%|███▍      | 1800/5189 [17:02<31:29,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1800 bytes=475637 mtime=Sun Jan  4 15:35:09 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  40%|████      | 2100/5189 [20:07<30:41,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2100 bytes=551624 mtime=Sun Jan  4 15:38:14 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  46%|████▌     | 2373/5189 [21:54<14:55,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2373 bytes=646767 mtime=Sun Jan  4 15:40:01 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  52%|█████▏    | 2673/5189 [23:34<16:19,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2673 bytes=724471 mtime=Sun Jan  4 15:41:41 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  57%|█████▋    | 2973/5189 [26:02<19:04,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2973 bytes=846579 mtime=Sun Jan  4 15:44:09 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  63%|██████▎   | 3273/5189 [30:51<18:26,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3272 bytes=975628 mtime=Sun Jan  4 15:48:58 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  69%|██████▉   | 3573/5189 [35:17<19:51,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3572 bytes=1182508 mtime=Sun Jan  4 15:53:24 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  75%|███████▍  | 3873/5189 [41:32<39:47,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3870 bytes=1271571 mtime=Sun Jan  4 15:59:39 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  80%|████████  | 4173/5189 [1:27:13<2:06:03,  7.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4095 bytes=1325755 mtime=Sun Jan  4 16:45:20 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  86%|████████▌ | 4473/5189 [1:33:00<16:44,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4395 bytes=1406987 mtime=Sun Jan  4 16:51:07 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  92%|█████████▏| 4773/5189 [2:05:43<1:10:07, 10.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4664 bytes=1497349 mtime=Sun Jan  4 17:23:50 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  98%|█████████▊| 5073/5189 [2:35:15<12:22,  6.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4936 bytes=1623168 mtime=Sun Jan  4 17:53:22 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 5189/5189 [3:00:32<00:00,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL SAVE] rows=5029 bytes=1674570 mtime=Sun Jan  4 18:18:38 2026\n",
            "Failures logged to: /content/drive/MyDrive/translation_shards/shard_person4_S0_zho_mya_fas_failures.jsonl count= 160\n",
            "DONE. Shard saved: /content/drive/MyDrive/translation_shards/shard_person4_S0_zho_mya_fas.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}