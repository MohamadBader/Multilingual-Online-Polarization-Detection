{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIUvSFU524gp",
        "outputId": "843b1ba4-1cf5-428c-c39d-dbf5c0e01132"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kBLw2xlK09o3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL TEAM TRANSLATION SCRIPT (DRIVE-SAFE + NO UNNEEDED API)\n",
        "# - Copy-only for TEACHER_LANGS (NO API calls)\n",
        "# - Translate only selected languages for THIS run\n",
        "# - Team sharding: 5 people + optional 2 sessions/person (2 notebooks/keys)\n",
        "# - High-quality prompts (similar to your old logic)\n",
        "# - Marker protection + validation (no new @/#/URLs)\n",
        "# - Batched translation (faster)\n",
        "# - DURABLE checkpointing to Google Drive (atomic write + fsync)\n",
        "# - Resume-safe (skips already translated row_ids in shard file)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# 1) CONFIG\n",
        "# =========================\n",
        "\n",
        "# Input master (original)\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "\n",
        "# Output directory for shards (each run writes its own file)\n",
        "OUT_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Output column name\n",
        "OUT_COL = \"text_en\"\n",
        "\n",
        "# Master columns\n",
        "TEXT_COL = \"text\"\n",
        "LANG_COL = \"lang\"\n",
        "SPLIT_COL = \"split\"\n",
        "\n",
        "# Translate only train by default (recommended)\n",
        "TRANSLATE_SPLITS = {\"train\"}  # change to {\"train\",\"dev\"} if you really want dev too\n",
        "\n",
        "# Teacher languages: copy-only (no API calls)\n",
        "TEACHER_LANGS = {\"eng\", \"spa\", \"deu\", \"rus\", \"tur\", \"pol\", \"arb\"}\n",
        "\n",
        "# Language names (for prompt)\n",
        "LANG_MAP = {\n",
        "    \"amh\": \"Amharic\", \"arb\": \"Arabic\", \"ben\": \"Bengali\", \"deu\": \"German\", \"eng\": \"English\",\n",
        "    \"fas\": \"Persian\", \"hau\": \"Hausa\", \"hin\": \"Hindi\", \"ita\": \"Italian\", \"khm\": \"Khmer\",\n",
        "    \"mya\": \"Burmese\", \"nep\": \"Nepali\", \"ori\": \"Odia\", \"pan\": \"Punjabi\", \"pol\": \"Polish\",\n",
        "    \"rus\": \"Russian\", \"spa\": \"Spanish\", \"swa\": \"Swahili\", \"tel\": \"Telugu\", \"tur\": \"Turkish\",\n",
        "    \"urd\": \"Urdu\", \"zho\": \"Chinese\"\n",
        "}\n",
        "\n",
        "# Team sharding\n",
        "N_PEOPLE = 5\n",
        "PERSON_ID = 3          # <-- each person sets 0..4\n",
        "\n",
        "# Two sessions per person:\n",
        "# Run the SAME script in 2 separate notebooks, each with a different API key:\n",
        "# - Notebook A: SESSION_ID=0\n",
        "# - Notebook B: SESSION_ID=1\n",
        "SESSION_ID = 1         # <-- set to 0 or 1 (or None if you don't want 2 sessions)\n",
        "N_SESSIONS_PER_PERSON = 2\n",
        "\n",
        "# Which languages THIS run should translate (non-teacher langs recommended)\n",
        "PERSON_LANGS = {\n",
        "  0: [\"urd\", \"hin\", \"hau\"],\n",
        "  1: [\"khm\", \"nep\", \"tel\"],\n",
        "  2: [\"ben\", \"amh\", \"ita\"],\n",
        "  3: [\"swa\", \"ori\", \"pan\"],\n",
        "  4: [\"zho\", \"mya\", \"fas\"],\n",
        "}\n",
        "LANGS_THIS_RUN = PERSON_LANGS[PERSON_ID]\n",
        "\n",
        "\n",
        "# Test mode\n",
        "TEST_MODE = False\n",
        "TEST_SAMPLES_PER_LANG = 5\n",
        "\n",
        "# OpenAI\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"PRIVATE\")\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Batching + saving\n",
        "BATCH_SIZE = 30      # 20–50 good. Use 10 for debugging saving\n",
        "SAVE_EVERY = 10      # save every N batches. Use 1 for debugging saving\n",
        "\n",
        "# Retry\n",
        "MAX_RETRIES = 4\n",
        "BASE_BACKOFF = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) MARKER PROTECTION (same spirit as your old code)\n",
        "# =========================\n",
        "\n",
        "MARKER_PATTERNS = [\n",
        "    r'@\\w+',              # mentions\n",
        "    r'#\\w+',              # hashtags\n",
        "    r'https?://\\S+',      # URLs\n",
        "    r'www\\.\\S+',          # URLs\n",
        "]\n",
        "\n",
        "def extract_markers(text: str) -> set:\n",
        "    out = set()\n",
        "    for pat in MARKER_PATTERNS:\n",
        "        out.update(re.findall(pat, str(text)))\n",
        "    return out\n",
        "\n",
        "def stable_row_id(lang: str, text: str) -> str:\n",
        "    h = hashlib.sha256((str(lang) + \"||\" + str(text)).encode(\"utf-8\")).hexdigest()\n",
        "    return h[:16]\n",
        "\n",
        "def validate_translation(seed_text: str, translated: str) -> Tuple[bool, str]:\n",
        "    if not isinstance(translated, str) or len(translated.strip()) < 2:\n",
        "        return False, \"empty translation\"\n",
        "\n",
        "    # Marker constraint: translated must not introduce new markers\n",
        "    seed_markers = {m.lower() for m in extract_markers(seed_text)}\n",
        "    seed_markers.discard(\"@user\")  # allow @USER normalization\n",
        "    out_markers = {m.lower() for m in extract_markers(translated)}\n",
        "    out_markers.discard(\"@user\")\n",
        "\n",
        "    extra = out_markers - seed_markers\n",
        "    if extra:\n",
        "        return False, f\"introduced new marker(s): {list(extra)[:3]}\"\n",
        "\n",
        "    return True, \"ok\""
      ],
      "metadata": {
        "id": "MI2b-HFr1Mx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) PROMPTS (high-quality, context-aware)\n",
        "# =========================\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a professional human translator and localization expert.\\n\"\n",
        "    \"Translate social media text into natural, fluent English while preserving meaning, tone, and pragmatic intent.\\n\"\n",
        "    \"Use culturally appropriate equivalents; do NOT translate word-by-word if it harms meaning.\\n\"\n",
        "    \"Do NOT add explanations. Return ONLY valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE_SINGLE = \"\"\"Translate the following social media post into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Preserve the original meaning, stance, and tone (including sarcasm, slang, dialect, irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor or soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Keep formatting minimal and readable.\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translation\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Batched version: returns list of translations aligned with inputs\n",
        "USER_TEMPLATE_BATCH = \"\"\"Translate the following social media posts into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "CRITICAL REQUIREMENTS (apply to every item):\n",
        "- Preserve meaning, stance, and tone (sarcasm/slang/dialect/irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor/soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Return EXACTLY one translation per input, same order.\n",
        "\n",
        "INPUTS (JSON array of strings):\n",
        "{inputs_json}\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translations\": [\"...\", \"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# 4) SHARDING (deterministic)\n",
        "# =========================\n",
        "\n",
        "def assign_person(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_PEOPLE\n",
        "\n",
        "def assign_session(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_SESSIONS_PER_PERSON\n",
        "\n",
        "def chunk_list(xs: List[Any], n: int) -> List[List[Any]]:\n",
        "    return [xs[i:i+n] for i in range(0, len(xs), n)]\n",
        "\n",
        "# =========================\n",
        "# 5) DURABLE SAVE (atomic + fsync)\n",
        "# =========================\n",
        "\n",
        "def flush_shard(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    pd.DataFrame(rows).to_csv(tmp, index=False)\n",
        "\n",
        "    # force flush to disk\n",
        "    with open(tmp, \"rb\") as f:\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# =========================\n",
        "# 6) OPENAI CALL (batch + retry + validation)\n",
        "# =========================\n",
        "\n",
        "def call_openai_translate_batch(\n",
        "    client: OpenAI,\n",
        "    lang_code: str,\n",
        "    seed_texts: List[str],\n",
        ") -> List[str]:\n",
        "    lang_name = LANG_MAP.get(lang_code, \"Unknown\")\n",
        "    user_prompt = USER_TEMPLATE_BATCH.format(\n",
        "        source_lang_name=lang_name,\n",
        "        lang_code=lang_code,\n",
        "        inputs_json=json.dumps(seed_texts, ensure_ascii=False)\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.2\n",
        "            )\n",
        "            data = json.loads(resp.choices[0].message.content)\n",
        "            outs = data.get(\"translations\", None)\n",
        "\n",
        "            if not isinstance(outs, list) or len(outs) != len(seed_texts):\n",
        "                raise ValueError(f\"Bad JSON: expected translations list of len {len(seed_texts)}\")\n",
        "\n",
        "            # Validate each translation (marker protection)\n",
        "            cleaned = []\n",
        "            for seed, tr in zip(seed_texts, outs):\n",
        "                ok, reason = validate_translation(seed, tr)\n",
        "                if not ok:\n",
        "                    raise ValueError(f\"Validation failed: {reason}\")\n",
        "                cleaned.append(str(tr).strip())\n",
        "\n",
        "            return cleaned\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == MAX_RETRIES:\n",
        "                break\n",
        "            time.sleep(BASE_BACKOFF * (2 ** (attempt - 1)) + 0.25 * attempt)\n",
        "\n",
        "    raise RuntimeError(f\"Translation failed after retries: {last_err}\")\n",
        "\n",
        "def safe_read_csv(path: str) -> pd.DataFrame:\n",
        "    if (not os.path.exists(path)) or os.path.getsize(path) < 5:\n",
        "        # empty or non-existent -> treat as no previous progress\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path)\n",
        "# =========================\n",
        "# 7) MAIN\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # 1) Load\n",
        "    df = pd.read_csv(MASTER_IN)\n",
        "\n",
        "    # 2) Ensure columns\n",
        "    if OUT_COL not in df.columns:\n",
        "        df[OUT_COL] = pd.NA\n",
        "\n",
        "    if \"row_id\" not in df.columns:\n",
        "        df[\"row_id\"] = [stable_row_id(l, t) for l, t in zip(df[LANG_COL], df[TEXT_COL])]\n",
        "\n",
        "    # 3) Copy-only teacher langs (NO API)\n",
        "    teacher_mask = df[LANG_COL].isin(TEACHER_LANGS)\n",
        "    df.loc[teacher_mask & df[OUT_COL].isna(), OUT_COL] = df.loc[teacher_mask & df[OUT_COL].isna(), TEXT_COL]\n",
        "\n",
        "    # 4) Work mask: chosen splits + langs + missing + not teacher\n",
        "    split_mask = df[SPLIT_COL].astype(str).isin(TRANSLATE_SPLITS)\n",
        "    lang_mask = df[LANG_COL].isin(LANGS_THIS_RUN)\n",
        "    need_mask = split_mask & lang_mask & (~teacher_mask) & (df[OUT_COL].isna())\n",
        "\n",
        "    # 5) Apply team sharding (ONLY if PERSON_ID / SESSION_ID are set)\n",
        "    idx = df.index[need_mask].tolist()\n",
        "\n",
        "    if SESSION_ID is not None:\n",
        "        idx = [i for i in idx if assign_session(str(df.at[i, \"row_id\"])) == SESSION_ID]\n",
        "\n",
        "\n",
        "    # 6) TEST_MODE sampling\n",
        "    if TEST_MODE:\n",
        "        sampled = []\n",
        "        for lg in LANGS_THIS_RUN:\n",
        "            candidates = [i for i in idx if df.at[i, LANG_COL] == lg]\n",
        "            sampled.extend(candidates[:TEST_SAMPLES_PER_LANG])\n",
        "        idx = sampled\n",
        "\n",
        "    # 7) Output path for shard (safe even when PERSON_ID/SESSION_ID is None)\n",
        "    person_tag = f\"person{PERSON_ID}\" if PERSON_ID is not None else \"personALL\"\n",
        "    sess_tag   = f\"S{SESSION_ID}\"     if SESSION_ID is not None else \"SALL\"\n",
        "\n",
        "    out_path = os.path.join(\n",
        "        OUT_DIR,\n",
        "        f\"shard_{person_tag}_{sess_tag}_\" + \"_\".join(LANGS_THIS_RUN) + \".csv\"\n",
        "    )\n",
        "    print(\"Shard output path:\", out_path)\n",
        "\n",
        "\n",
        "    # 8) Resume support (robust to empty/partial files)\n",
        "    done_row_ids = set()\n",
        "    shard_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    prev = safe_read_csv(out_path)\n",
        "    if not prev.empty:\n",
        "        # only accept if it actually has the expected columns\n",
        "        if \"row_id\" in prev.columns and OUT_COL in prev.columns:\n",
        "            done_row_ids = set(prev.loc[prev[OUT_COL].notna(), \"row_id\"].astype(str).tolist())\n",
        "        shard_rows = prev.to_dict(\"records\")\n",
        "        print(\"Resume: already translated in shard:\", len(done_row_ids))\n",
        "    else:\n",
        "        print(\"Resume: no valid previous shard (file missing or empty). Starting fresh.\")\n",
        "\n",
        "    idx = [i for i in idx if str(df.at[i, \"row_id\"]) not in done_row_ids]\n",
        "    print(\"Total master rows:\", len(df))\n",
        "    print(\"Teacher rows (copy-only) missing text_en:\", int((teacher_mask & df[OUT_COL].isna()).sum()))\n",
        "    print(\"Rows assigned to THIS run (after resume):\", len(idx))\n",
        "\n",
        "    if len(idx) == 0:\n",
        "        # still save shard (might contain previous work)\n",
        "        if shard_rows:\n",
        "            flush_shard(shard_rows, out_path)\n",
        "            print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "        else:\n",
        "            print(\"[FINAL SAVE] skipped (no rows)\")\n",
        "\n",
        "\n",
        "    # 9) Group by language (batch per language)\n",
        "    groups: Dict[str, List[int]] = {}\n",
        "    for i in idx:\n",
        "        lg = str(df.at[i, LANG_COL])\n",
        "        groups.setdefault(lg, []).append(i)\n",
        "\n",
        "    # 10) Client\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # 11) Translate\n",
        "    total = sum(len(v) for v in groups.values())\n",
        "    pbar = tqdm(total=total, desc=\"Translating\")\n",
        "    batches_since_save = 0\n",
        "\n",
        "    failures = []\n",
        "    try:\n",
        "        for lg, rows in groups.items():\n",
        "            for chunk in chunk_list(rows, BATCH_SIZE):\n",
        "                seed_texts = [str(df.at[i, TEXT_COL]) for i in chunk]\n",
        "\n",
        "                try:\n",
        "                    outs = call_openai_translate_batch(client, lg, seed_texts)\n",
        "                except Exception as e:\n",
        "                    # fall back to single-item calls (debug + salvage)\n",
        "                    outs = []\n",
        "                    for one_i, one_seed in zip(chunk, seed_texts):\n",
        "                        try:\n",
        "                            # single prompt fallback\n",
        "                            lang_name = LANG_MAP.get(lg, \"Unknown\")\n",
        "                            user_prompt = USER_TEMPLATE_SINGLE.format(\n",
        "                                source_lang_name=lang_name,\n",
        "                                lang_code=lg,\n",
        "                                text=one_seed\n",
        "                            )\n",
        "                            resp = client.chat.completions.create(\n",
        "                                model=MODEL,\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                                ],\n",
        "                                response_format={\"type\": \"json_object\"},\n",
        "                                temperature=0.2\n",
        "                            )\n",
        "                            data = json.loads(resp.choices[0].message.content)\n",
        "                            tr = data.get(\"translation\", \"\")\n",
        "                            ok, reason = validate_translation(one_seed, tr)\n",
        "                            if not ok:\n",
        "                                raise ValueError(reason)\n",
        "                            outs.append(str(tr).strip())\n",
        "                        except Exception as e2:\n",
        "                            failures.append({\n",
        "                                \"row_id\": str(df.at[one_i, \"row_id\"]),\n",
        "                                \"lang\": lg,\n",
        "                                \"error\": str(e2),\n",
        "                                \"seed_text\": one_seed[:500]\n",
        "                            })\n",
        "                            outs.append(None)\n",
        "\n",
        "                # write successes\n",
        "                for i, tr in zip(chunk, outs):\n",
        "                    if tr is None:\n",
        "                        continue\n",
        "                    df.at[i, OUT_COL] = tr\n",
        "                    shard_rows.append({\n",
        "                        \"row_id\": str(df.at[i, \"row_id\"]),\n",
        "                        LANG_COL: str(df.at[i, LANG_COL]),\n",
        "                        SPLIT_COL: str(df.at[i, SPLIT_COL]),\n",
        "                        TEXT_COL: str(df.at[i, TEXT_COL]),\n",
        "                        OUT_COL: tr\n",
        "                    })\n",
        "\n",
        "                pbar.update(len(chunk))\n",
        "                batches_since_save += 1\n",
        "\n",
        "                if batches_since_save >= SAVE_EVERY:\n",
        "                    if shard_rows:\n",
        "                        flush_shard(shard_rows, out_path)\n",
        "                        print(f\"[SAVED] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "                    else:\n",
        "                        print(\"[SAVED] skipped (no rows yet)\")\n",
        "                    batches_since_save = 0\n",
        "\n",
        "\n",
        "        # final flush\n",
        "        flush_shard(shard_rows, out_path)\n",
        "        print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "\n",
        "        # failures log\n",
        "        if failures:\n",
        "            fail_path = out_path.replace(\".csv\", \"_failures.jsonl\")\n",
        "            with open(fail_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for r in failures:\n",
        "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "            print(\"Failures logged to:\", fail_path, \"count=\", len(failures))\n",
        "\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "    # TEST_MODE preview\n",
        "    if TEST_MODE:\n",
        "        print(\"\\n=== TEST MODE PREVIEW ===\")\n",
        "        if shard_rows:\n",
        "            preview = pd.DataFrame(shard_rows).tail(10)\n",
        "            print(preview[[LANG_COL, TEXT_COL, OUT_COL]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"(empty) No successful translations generated.\")\n",
        "\n",
        "    print(\"DONE. Shard saved:\", out_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTXKbfW51Wit",
        "outputId": "0183f44f-234a-49e1-c3fb-aea53749b5f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shard output path: /content/drive/MyDrive/translation_shards/shard_person3_S1_swa_ori_pan.csv\n",
            "Resume: no valid previous shard (file missing or empty). Starting fresh.\n",
            "Total master rows: 77368\n",
            "Teacher rows (copy-only) missing text_en: 0\n",
            "Rows assigned to THIS run (after resume): 5521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:   5%|▌         | 300/5521 [14:27<4:33:43,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=286 bytes=59438 mtime=Sun Jan  4 15:14:40 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  11%|█         | 600/5521 [18:29<40:12,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=584 bytes=116518 mtime=Sun Jan  4 15:18:41 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  16%|█▌        | 881/5521 [20:40<36:49,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=865 bytes=180982 mtime=Sun Jan  4 15:20:52 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  21%|██▏       | 1181/5521 [22:51<30:44,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1165 bytes=236767 mtime=Sun Jan  4 15:23:04 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  27%|██▋       | 1481/5521 [25:57<29:08,  2.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1465 bytes=292551 mtime=Sun Jan  4 15:26:10 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  32%|███▏      | 1781/5521 [31:50<1:14:24,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1759 bytes=345617 mtime=Sun Jan  4 15:32:03 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  38%|███▊      | 2081/5521 [39:17<2:20:37,  2.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2031 bytes=393290 mtime=Sun Jan  4 15:39:29 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  43%|████▎     | 2381/5521 [48:04<49:26,  1.06it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2274 bytes=438099 mtime=Sun Jan  4 15:48:17 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  49%|████▊     | 2681/5521 [52:54<29:13,  1.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2572 bytes=489858 mtime=Sun Jan  4 15:53:06 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  54%|█████▍    | 2981/5521 [59:04<38:25,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2860 bytes=550987 mtime=Sun Jan  4 15:59:17 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  59%|█████▉    | 3281/5521 [1:01:24<17:52,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3160 bytes=607855 mtime=Sun Jan  4 16:01:36 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  65%|██████▍   | 3581/5521 [1:04:10<18:23,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3460 bytes=666166 mtime=Sun Jan  4 16:04:23 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  70%|███████   | 3881/5521 [1:08:26<15:39,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3760 bytes=721642 mtime=Sun Jan  4 16:08:38 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  76%|███████▌  | 4181/5521 [1:14:20<29:51,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4057 bytes=781072 mtime=Sun Jan  4 16:14:33 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  81%|████████  | 4457/5521 [1:22:08<22:37,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4325 bytes=843193 mtime=Sun Jan  4 16:22:21 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  86%|████████▌ | 4757/5521 [1:35:04<44:04,  3.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4619 bytes=983601 mtime=Sun Jan  4 16:35:17 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  92%|█████████▏| 5057/5521 [1:38:03<04:55,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4919 bytes=1134920 mtime=Sun Jan  4 16:38:16 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  97%|█████████▋| 5357/5521 [1:40:41<01:33,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=5219 bytes=1283603 mtime=Sun Jan  4 16:40:53 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 5521/5521 [1:42:11<00:00,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL SAVE] rows=5383 bytes=1365066 mtime=Sun Jan  4 16:42:24 2026\n",
            "Failures logged to: /content/drive/MyDrive/translation_shards/shard_person3_S1_swa_ori_pan_failures.jsonl count= 138\n",
            "DONE. Shard saved: /content/drive/MyDrive/translation_shards/shard_person3_S1_swa_ori_pan.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}