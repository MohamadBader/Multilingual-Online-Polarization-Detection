{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIUvSFU524gp",
        "outputId": "5cf081ed-ce50-4349-aa7b-aec04aa38aca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kBLw2xlK09o3"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL TEAM TRANSLATION SCRIPT (DRIVE-SAFE + NO UNNEEDED API)\n",
        "# - Copy-only for TEACHER_LANGS (NO API calls)\n",
        "# - Translate only selected languages for THIS run\n",
        "# - Team sharding: 5 people + optional 2 sessions/person (2 notebooks/keys)\n",
        "# - High-quality prompts (similar to your old logic)\n",
        "# - Marker protection + validation (no new @/#/URLs)\n",
        "# - Batched translation (faster)\n",
        "# - DURABLE checkpointing to Google Drive (atomic write + fsync)\n",
        "# - Resume-safe (skips already translated row_ids in shard file)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# =========================\n",
        "# 1) CONFIG\n",
        "# =========================\n",
        "\n",
        "# Input master (original)\n",
        "MASTER_IN = \"/content/drive/MyDrive/master_dataset.csv\"\n",
        "\n",
        "# Output directory for shards (each run writes its own file)\n",
        "OUT_DIR = \"/content/drive/MyDrive/translation_shards\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Output column name\n",
        "OUT_COL = \"text_en\"\n",
        "\n",
        "# Master columns\n",
        "TEXT_COL = \"text\"\n",
        "LANG_COL = \"lang\"\n",
        "SPLIT_COL = \"split\"\n",
        "\n",
        "# Translate only train by default (recommended)\n",
        "TRANSLATE_SPLITS = {\"train\"}  # change to {\"train\",\"dev\"} if you really want dev too\n",
        "\n",
        "# Teacher languages: copy-only (no API calls)\n",
        "TEACHER_LANGS = {\"eng\", \"spa\", \"deu\", \"rus\", \"tur\", \"pol\", \"arb\"}\n",
        "\n",
        "# Language names (for prompt)\n",
        "LANG_MAP = {\n",
        "    \"amh\": \"Amharic\", \"arb\": \"Arabic\", \"ben\": \"Bengali\", \"deu\": \"German\", \"eng\": \"English\",\n",
        "    \"fas\": \"Persian\", \"hau\": \"Hausa\", \"hin\": \"Hindi\", \"ita\": \"Italian\", \"khm\": \"Khmer\",\n",
        "    \"mya\": \"Burmese\", \"nep\": \"Nepali\", \"ori\": \"Odia\", \"pan\": \"Punjabi\", \"pol\": \"Polish\",\n",
        "    \"rus\": \"Russian\", \"spa\": \"Spanish\", \"swa\": \"Swahili\", \"tel\": \"Telugu\", \"tur\": \"Turkish\",\n",
        "    \"urd\": \"Urdu\", \"zho\": \"Chinese\"\n",
        "}\n",
        "\n",
        "# Team sharding\n",
        "N_PEOPLE = 5\n",
        "PERSON_ID = 2          # <-- each person sets 0..4\n",
        "\n",
        "# Two sessions per person:\n",
        "# Run the SAME script in 2 separate notebooks, each with a different API key:\n",
        "# - Notebook A: SESSION_ID=0\n",
        "# - Notebook B: SESSION_ID=1\n",
        "SESSION_ID = 1         # <-- set to 0 or 1 (or None if you don't want 2 sessions)\n",
        "N_SESSIONS_PER_PERSON = 2\n",
        "\n",
        "# Which languages THIS run should translate (non-teacher langs recommended)\n",
        "PERSON_LANGS = {\n",
        "  0: [\"urd\", \"hin\", \"hau\"],\n",
        "  1: [\"khm\", \"nep\", \"tel\"],\n",
        "  2: [\"ben\", \"amh\", \"ita\"],\n",
        "  3: [\"swa\", \"ori\", \"pan\"],\n",
        "  4: [\"zho\", \"mya\", \"fas\"],\n",
        "}\n",
        "LANGS_THIS_RUN = PERSON_LANGS[PERSON_ID]\n",
        "\n",
        "\n",
        "# Test mode\n",
        "TEST_MODE = False\n",
        "TEST_SAMPLES_PER_LANG = 5\n",
        "\n",
        "# OpenAI\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"PRIVATE\")\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Batching + saving\n",
        "BATCH_SIZE = 30      # 20–50 good. Use 10 for debugging saving\n",
        "SAVE_EVERY = 10      # save every N batches. Use 1 for debugging saving\n",
        "\n",
        "# Retry\n",
        "MAX_RETRIES = 4\n",
        "BASE_BACKOFF = 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) MARKER PROTECTION (same spirit as your old code)\n",
        "# =========================\n",
        "\n",
        "MARKER_PATTERNS = [\n",
        "    r'@\\w+',              # mentions\n",
        "    r'#\\w+',              # hashtags\n",
        "    r'https?://\\S+',      # URLs\n",
        "    r'www\\.\\S+',          # URLs\n",
        "]\n",
        "\n",
        "def extract_markers(text: str) -> set:\n",
        "    out = set()\n",
        "    for pat in MARKER_PATTERNS:\n",
        "        out.update(re.findall(pat, str(text)))\n",
        "    return out\n",
        "\n",
        "def stable_row_id(lang: str, text: str) -> str:\n",
        "    h = hashlib.sha256((str(lang) + \"||\" + str(text)).encode(\"utf-8\")).hexdigest()\n",
        "    return h[:16]\n",
        "\n",
        "def validate_translation(seed_text: str, translated: str) -> Tuple[bool, str]:\n",
        "    if not isinstance(translated, str) or len(translated.strip()) < 2:\n",
        "        return False, \"empty translation\"\n",
        "\n",
        "    # Marker constraint: translated must not introduce new markers\n",
        "    seed_markers = {m.lower() for m in extract_markers(seed_text)}\n",
        "    seed_markers.discard(\"@user\")  # allow @USER normalization\n",
        "    out_markers = {m.lower() for m in extract_markers(translated)}\n",
        "    out_markers.discard(\"@user\")\n",
        "\n",
        "    extra = out_markers - seed_markers\n",
        "    if extra:\n",
        "        return False, f\"introduced new marker(s): {list(extra)[:3]}\"\n",
        "\n",
        "    return True, \"ok\""
      ],
      "metadata": {
        "id": "MI2b-HFr1Mx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) PROMPTS (high-quality, context-aware)\n",
        "# =========================\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a professional human translator and localization expert.\\n\"\n",
        "    \"Translate social media text into natural, fluent English while preserving meaning, tone, and pragmatic intent.\\n\"\n",
        "    \"Use culturally appropriate equivalents; do NOT translate word-by-word if it harms meaning.\\n\"\n",
        "    \"Do NOT add explanations. Return ONLY valid JSON.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE_SINGLE = \"\"\"Translate the following social media post into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Preserve the original meaning, stance, and tone (including sarcasm, slang, dialect, irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor or soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Keep formatting minimal and readable.\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translation\": \"...\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Batched version: returns list of translations aligned with inputs\n",
        "USER_TEMPLATE_BATCH = \"\"\"Translate the following social media posts into English.\n",
        "\n",
        "Source language: {source_lang_name} ({lang_code})\n",
        "\n",
        "CRITICAL REQUIREMENTS (apply to every item):\n",
        "- Preserve meaning, stance, and tone (sarcasm/slang/dialect/irony).\n",
        "- Do NOT add new information.\n",
        "- Do NOT censor/soften content; translate faithfully.\n",
        "- Preserve ALL @mentions, #hashtags, and URLs EXACTLY as in the original (do not introduce new ones).\n",
        "- Return EXACTLY one translation per input, same order.\n",
        "\n",
        "INPUTS (JSON array of strings):\n",
        "{inputs_json}\n",
        "\n",
        "Output JSON only:\n",
        "{{\n",
        "  \"translations\": [\"...\", \"...\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# 4) SHARDING (deterministic)\n",
        "# =========================\n",
        "\n",
        "def assign_person(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_PEOPLE\n",
        "\n",
        "def assign_session(row_id_hex: str) -> int:\n",
        "    return int(row_id_hex, 16) % N_SESSIONS_PER_PERSON\n",
        "\n",
        "def chunk_list(xs: List[Any], n: int) -> List[List[Any]]:\n",
        "    return [xs[i:i+n] for i in range(0, len(xs), n)]\n",
        "\n",
        "# =========================\n",
        "# 5) DURABLE SAVE (atomic + fsync)\n",
        "# =========================\n",
        "\n",
        "def flush_shard(rows: List[Dict[str, Any]], path: str) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    tmp = path + \".tmp\"\n",
        "    pd.DataFrame(rows).to_csv(tmp, index=False)\n",
        "\n",
        "    # force flush to disk\n",
        "    with open(tmp, \"rb\") as f:\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "# =========================\n",
        "# 6) OPENAI CALL (batch + retry + validation)\n",
        "# =========================\n",
        "\n",
        "def call_openai_translate_batch(\n",
        "    client: OpenAI,\n",
        "    lang_code: str,\n",
        "    seed_texts: List[str],\n",
        ") -> List[str]:\n",
        "    lang_name = LANG_MAP.get(lang_code, \"Unknown\")\n",
        "    user_prompt = USER_TEMPLATE_BATCH.format(\n",
        "        source_lang_name=lang_name,\n",
        "        lang_code=lang_code,\n",
        "        inputs_json=json.dumps(seed_texts, ensure_ascii=False)\n",
        "    )\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.2\n",
        "            )\n",
        "            data = json.loads(resp.choices[0].message.content)\n",
        "            outs = data.get(\"translations\", None)\n",
        "\n",
        "            if not isinstance(outs, list) or len(outs) != len(seed_texts):\n",
        "                raise ValueError(f\"Bad JSON: expected translations list of len {len(seed_texts)}\")\n",
        "\n",
        "            # Validate each translation (marker protection)\n",
        "            cleaned = []\n",
        "            for seed, tr in zip(seed_texts, outs):\n",
        "                ok, reason = validate_translation(seed, tr)\n",
        "                if not ok:\n",
        "                    raise ValueError(f\"Validation failed: {reason}\")\n",
        "                cleaned.append(str(tr).strip())\n",
        "\n",
        "            return cleaned\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt == MAX_RETRIES:\n",
        "                break\n",
        "            time.sleep(BASE_BACKOFF * (2 ** (attempt - 1)) + 0.25 * attempt)\n",
        "\n",
        "    raise RuntimeError(f\"Translation failed after retries: {last_err}\")\n",
        "\n",
        "def safe_read_csv(path: str) -> pd.DataFrame:\n",
        "    if (not os.path.exists(path)) or os.path.getsize(path) < 5:\n",
        "        # empty or non-existent -> treat as no previous progress\n",
        "        return pd.DataFrame()\n",
        "    return pd.read_csv(path)\n",
        "# =========================\n",
        "# 7) MAIN\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # 1) Load\n",
        "    df = pd.read_csv(MASTER_IN)\n",
        "\n",
        "    # 2) Ensure columns\n",
        "    if OUT_COL not in df.columns:\n",
        "        df[OUT_COL] = pd.NA\n",
        "\n",
        "    if \"row_id\" not in df.columns:\n",
        "        df[\"row_id\"] = [stable_row_id(l, t) for l, t in zip(df[LANG_COL], df[TEXT_COL])]\n",
        "\n",
        "    # 3) Copy-only teacher langs (NO API)\n",
        "    teacher_mask = df[LANG_COL].isin(TEACHER_LANGS)\n",
        "    df.loc[teacher_mask & df[OUT_COL].isna(), OUT_COL] = df.loc[teacher_mask & df[OUT_COL].isna(), TEXT_COL]\n",
        "\n",
        "    # 4) Work mask: chosen splits + langs + missing + not teacher\n",
        "    split_mask = df[SPLIT_COL].astype(str).isin(TRANSLATE_SPLITS)\n",
        "    lang_mask = df[LANG_COL].isin(LANGS_THIS_RUN)\n",
        "    need_mask = split_mask & lang_mask & (~teacher_mask) & (df[OUT_COL].isna())\n",
        "\n",
        "    # 5) Apply team sharding (ONLY if PERSON_ID / SESSION_ID are set)\n",
        "    idx = df.index[need_mask].tolist()\n",
        "\n",
        "    if SESSION_ID is not None:\n",
        "        idx = [i for i in idx if assign_session(str(df.at[i, \"row_id\"])) == SESSION_ID]\n",
        "\n",
        "\n",
        "    # 6) TEST_MODE sampling\n",
        "    if TEST_MODE:\n",
        "        sampled = []\n",
        "        for lg in LANGS_THIS_RUN:\n",
        "            candidates = [i for i in idx if df.at[i, LANG_COL] == lg]\n",
        "            sampled.extend(candidates[:TEST_SAMPLES_PER_LANG])\n",
        "        idx = sampled\n",
        "\n",
        "    # 7) Output path for shard (safe even when PERSON_ID/SESSION_ID is None)\n",
        "    person_tag = f\"person{PERSON_ID}\" if PERSON_ID is not None else \"personALL\"\n",
        "    sess_tag   = f\"S{SESSION_ID}\"     if SESSION_ID is not None else \"SALL\"\n",
        "\n",
        "    out_path = os.path.join(\n",
        "        OUT_DIR,\n",
        "        f\"shard_{person_tag}_{sess_tag}_\" + \"_\".join(LANGS_THIS_RUN) + \".csv\"\n",
        "    )\n",
        "    print(\"Shard output path:\", out_path)\n",
        "\n",
        "\n",
        "    # 8) Resume support (robust to empty/partial files)\n",
        "    done_row_ids = set()\n",
        "    shard_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    prev = safe_read_csv(out_path)\n",
        "    if not prev.empty:\n",
        "        # only accept if it actually has the expected columns\n",
        "        if \"row_id\" in prev.columns and OUT_COL in prev.columns:\n",
        "            done_row_ids = set(prev.loc[prev[OUT_COL].notna(), \"row_id\"].astype(str).tolist())\n",
        "        shard_rows = prev.to_dict(\"records\")\n",
        "        print(\"Resume: already translated in shard:\", len(done_row_ids))\n",
        "    else:\n",
        "        print(\"Resume: no valid previous shard (file missing or empty). Starting fresh.\")\n",
        "\n",
        "    idx = [i for i in idx if str(df.at[i, \"row_id\"]) not in done_row_ids]\n",
        "    print(\"Total master rows:\", len(df))\n",
        "    print(\"Teacher rows (copy-only) missing text_en:\", int((teacher_mask & df[OUT_COL].isna()).sum()))\n",
        "    print(\"Rows assigned to THIS run (after resume):\", len(idx))\n",
        "\n",
        "    if len(idx) == 0:\n",
        "        # still save shard (might contain previous work)\n",
        "        if shard_rows:\n",
        "            flush_shard(shard_rows, out_path)\n",
        "            print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "        else:\n",
        "            print(\"[FINAL SAVE] skipped (no rows)\")\n",
        "\n",
        "\n",
        "    # 9) Group by language (batch per language)\n",
        "    groups: Dict[str, List[int]] = {}\n",
        "    for i in idx:\n",
        "        lg = str(df.at[i, LANG_COL])\n",
        "        groups.setdefault(lg, []).append(i)\n",
        "\n",
        "    # 10) Client\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    # 11) Translate\n",
        "    total = sum(len(v) for v in groups.values())\n",
        "    pbar = tqdm(total=total, desc=\"Translating\")\n",
        "    batches_since_save = 0\n",
        "\n",
        "    failures = []\n",
        "    try:\n",
        "        for lg, rows in groups.items():\n",
        "            for chunk in chunk_list(rows, BATCH_SIZE):\n",
        "                seed_texts = [str(df.at[i, TEXT_COL]) for i in chunk]\n",
        "\n",
        "                try:\n",
        "                    outs = call_openai_translate_batch(client, lg, seed_texts)\n",
        "                except Exception as e:\n",
        "                    # fall back to single-item calls (debug + salvage)\n",
        "                    outs = []\n",
        "                    for one_i, one_seed in zip(chunk, seed_texts):\n",
        "                        try:\n",
        "                            # single prompt fallback\n",
        "                            lang_name = LANG_MAP.get(lg, \"Unknown\")\n",
        "                            user_prompt = USER_TEMPLATE_SINGLE.format(\n",
        "                                source_lang_name=lang_name,\n",
        "                                lang_code=lg,\n",
        "                                text=one_seed\n",
        "                            )\n",
        "                            resp = client.chat.completions.create(\n",
        "                                model=MODEL,\n",
        "                                messages=[\n",
        "                                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                                ],\n",
        "                                response_format={\"type\": \"json_object\"},\n",
        "                                temperature=0.2\n",
        "                            )\n",
        "                            data = json.loads(resp.choices[0].message.content)\n",
        "                            tr = data.get(\"translation\", \"\")\n",
        "                            ok, reason = validate_translation(one_seed, tr)\n",
        "                            if not ok:\n",
        "                                raise ValueError(reason)\n",
        "                            outs.append(str(tr).strip())\n",
        "                        except Exception as e2:\n",
        "                            failures.append({\n",
        "                                \"row_id\": str(df.at[one_i, \"row_id\"]),\n",
        "                                \"lang\": lg,\n",
        "                                \"error\": str(e2),\n",
        "                                \"seed_text\": one_seed[:500]\n",
        "                            })\n",
        "                            outs.append(None)\n",
        "\n",
        "                # write successes\n",
        "                for i, tr in zip(chunk, outs):\n",
        "                    if tr is None:\n",
        "                        continue\n",
        "                    df.at[i, OUT_COL] = tr\n",
        "                    shard_rows.append({\n",
        "                        \"row_id\": str(df.at[i, \"row_id\"]),\n",
        "                        LANG_COL: str(df.at[i, LANG_COL]),\n",
        "                        SPLIT_COL: str(df.at[i, SPLIT_COL]),\n",
        "                        TEXT_COL: str(df.at[i, TEXT_COL]),\n",
        "                        OUT_COL: tr\n",
        "                    })\n",
        "\n",
        "                pbar.update(len(chunk))\n",
        "                batches_since_save += 1\n",
        "\n",
        "                if batches_since_save >= SAVE_EVERY:\n",
        "                    if shard_rows:\n",
        "                        flush_shard(shard_rows, out_path)\n",
        "                        print(f\"[SAVED] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "                    else:\n",
        "                        print(\"[SAVED] skipped (no rows yet)\")\n",
        "                    batches_since_save = 0\n",
        "\n",
        "\n",
        "        # final flush\n",
        "        flush_shard(shard_rows, out_path)\n",
        "        print(f\"[FINAL SAVE] rows={len(shard_rows)} bytes={os.path.getsize(out_path)} mtime={time.ctime(os.path.getmtime(out_path))}\")\n",
        "\n",
        "        # failures log\n",
        "        if failures:\n",
        "            fail_path = out_path.replace(\".csv\", \"_failures.jsonl\")\n",
        "            with open(fail_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                for r in failures:\n",
        "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "            print(\"Failures logged to:\", fail_path, \"count=\", len(failures))\n",
        "\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "    # TEST_MODE preview\n",
        "    if TEST_MODE:\n",
        "        print(\"\\n=== TEST MODE PREVIEW ===\")\n",
        "        if shard_rows:\n",
        "            preview = pd.DataFrame(shard_rows).tail(10)\n",
        "            print(preview[[LANG_COL, TEXT_COL, OUT_COL]].to_string(index=False))\n",
        "        else:\n",
        "            print(\"(empty) No successful translations generated.\")\n",
        "\n",
        "    print(\"DONE. Shard saved:\", out_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTXKbfW51Wit",
        "outputId": "eaa38949-8b58-4b2b-8d72-e29c3f5e0276"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shard output path: /content/drive/MyDrive/translation_shards/shard_person2_S1_ben_amh_ita.csv\n",
            "Resume: no valid previous shard (file missing or empty). Starting fresh.\n",
            "Total master rows: 77368\n",
            "Teacher rows (copy-only) missing text_en: 0\n",
            "Rows assigned to THIS run (after resume): 4946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:   6%|▌         | 300/4946 [21:41<5:52:33,  4.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=263 bytes=86329 mtime=Sun Jan  4 14:51:30 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  12%|█▏        | 600/4946 [44:27<5:07:33,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=525 bytes=172495 mtime=Sun Jan  4 15:14:17 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  18%|█▊        | 900/4946 [1:07:43<5:29:09,  4.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=788 bytes=256921 mtime=Sun Jan  4 15:37:32 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  24%|██▍       | 1200/4946 [1:32:03<5:17:49,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1048 bytes=340089 mtime=Sun Jan  4 16:01:52 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  30%|███       | 1500/4946 [1:55:48<4:11:58,  4.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1316 bytes=426807 mtime=Sun Jan  4 16:25:37 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  36%|███▋      | 1799/4946 [2:17:00<3:29:05,  3.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1577 bytes=502715 mtime=Sun Jan  4 16:46:50 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  42%|████▏     | 2099/4946 [2:38:29<3:31:06,  4.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=1845 bytes=604552 mtime=Sun Jan  4 17:08:18 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  49%|████▊     | 2399/4946 [2:49:52<1:51:44,  2.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2137 bytes=707465 mtime=Sun Jan  4 17:19:42 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  55%|█████▍    | 2699/4946 [3:07:59<2:26:36,  3.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2419 bytes=808706 mtime=Sun Jan  4 17:37:48 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  61%|██████    | 2999/4946 [3:26:16<1:40:03,  3.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2702 bytes=908031 mtime=Sun Jan  4 17:56:05 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  66%|██████▋   | 3285/4946 [3:40:48<1:20:15,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=2978 bytes=1011514 mtime=Sun Jan  4 18:10:37 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  72%|███████▏  | 3585/4946 [3:44:15<16:06,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3278 bytes=1200761 mtime=Sun Jan  4 18:14:04 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  79%|███████▊  | 3885/4946 [3:47:46<12:45,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3578 bytes=1387950 mtime=Sun Jan  4 18:17:36 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  85%|████████▍ | 4185/4946 [3:51:46<09:38,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=3878 bytes=1583007 mtime=Sun Jan  4 18:21:35 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  91%|█████████ | 4485/4946 [3:55:44<06:31,  1.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4178 bytes=1773867 mtime=Sun Jan  4 18:25:33 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating:  97%|█████████▋| 4785/4946 [4:00:19<02:49,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SAVED] rows=4478 bytes=1977375 mtime=Sun Jan  4 18:30:09 2026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 4946/4946 [4:03:14<00:00,  2.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FINAL SAVE] rows=4639 bytes=2087541 mtime=Sun Jan  4 18:33:03 2026\n",
            "Failures logged to: /content/drive/MyDrive/translation_shards/shard_person2_S1_ben_amh_ita_failures.jsonl count= 307\n",
            "DONE. Shard saved: /content/drive/MyDrive/translation_shards/shard_person2_S1_ben_amh_ita.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}